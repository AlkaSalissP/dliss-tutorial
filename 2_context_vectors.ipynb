{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-context-vectors.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/2_context_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGiURKhLmONv",
        "colab_type": "text"
      },
      "source": [
        "# Part II: Contextualized embeddings\n",
        "\n",
        "\n",
        "In this section, we are going to take load a pre-trained langage model (LM) checkpoint and use everything below the LM output as the lower layers of our previously defined classification model.  We dont really need to change anything else, we just need to pass this whole network as the `embedding` parameter to the model.\n",
        "\n",
        "\n",
        "## ELMo\n",
        "\n",
        "For this demo, we will focus on ELMo ([Peters et al 2018](https://export.arxiv.org/pdf/1802.05365)), a bidirectional LM, with an embedding layer and 2 subsequent biLSTM layers.  ELMo is a bidirectional generalization of the character-aware LM from [Kim et al 2015](https://arxiv.org/abs/1508.06615).  There is a nice [slide deck by the authors here](http://www.people.fas.harvard.edu/~yoonkim/data/char-nlm-slides.pdf), but the key high-level points are listed here:\n",
        "\n",
        "### Kim Language Model\n",
        "\n",
        "* **Goal**: predict the next word in the sentence (causal LM) but account for unseen words by using a character compositional approach that relies on letters within the pre-segmented words.  This also has the important impact of reducing the number of parameters required in the model drastically over word-level models.\n",
        "\n",
        "* **Using**: LSTM layers that take in a word representation for each position.  Each word is put in and used to predict the next word over a context\n",
        "\n",
        "* **The Twist**: use embeddings approach from [dos Santos & Zadrozny 2014](http://proceedings.mlr.press/v32/santos14.pdf) to represent words, but add parallel filters as in [Kim 2014](https://www.aclweb.org/anthology/D14-1181).  Also, add highway layers on top of the base model\n",
        "\n",
        "\n",
        "\n",
        "### ELMo Language Model\n",
        "\n",
        "* **Goal**: predict the next word in the sentence (causal LM) on the forward sequence **and** predict the previous word on the sentence conditioned on the following context.  \n",
        "\n",
        "* **Using**: LSTM layers as before, but bidirectional, sum the forward and backward loss to make one big loss\n",
        "\n",
        "* **The Twist** Potentially use all layers of the model (except we dont need head with the big softmax at the end over the words). After the fact, we can freeze our biLM embeddings but still provide useful information by learning a linear combination of the layers during downstream training.  During the biLM training, these scalars dont exist\n",
        "\n",
        "### ELMo with AllenNLP\n",
        "\n",
        "Even though ELMo is just a network like described above, there are a lot of details to getting it set up and reloading the pre-trained checkpoints that are provided, and these details are not really important for demonstration purposes.  So, we will just install [AllenNLP](https://github.com/allenai/allennlp) and use it the basis for a new embedding layer.\n",
        "\n",
        "If you are interested in learning more about using ELMo with AllenNLP, they have provided a [tutorial here](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md)\n",
        "\n",
        "#### TensorFlow and ELMo\n",
        "\n",
        "ELMo was originally trained with TensorFlow.  You can find the code to train and use it in the [bilm-tf repository](https://github.com/allenai/bilm-tf/tree/master/bilm)\n",
        "\n",
        "TF-Hub contains the [pre-trained ELMo model](https://tfhub.dev/google/elmo/2) and is very easy to integrate if you are using TensorFlow already.  The model takes a sequence of words (mixed-case) as inputs and can just be \"glued\" in to your existing models as a sub-graph of your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziMb2Iphr4vJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "255b06dc-0380-480d-da7d-ff294ab0ea4f"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/8c/72b14d20c9cbb0306939ea41109fc599302634fd5c59ccba1a659b7d0360/allennlp-0.8.4-py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 55.5MB/s \n",
            "\u001b[?25hCollecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/09/94d623dda1adacd51722f3e3e0f88ba08dd030ac2b2662bfb4383096340d/flaky-3.6.0-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d5/4458d7491971cdb2da8c3f679aa8c138cab6fe9b8f1d5519ae89751cdf44/awscli-1.16.186-py2.py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.167)\n",
            "Collecting jsonpickle (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 58.1MB/s \n",
            "\u001b[?25hCollecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.2)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/a8/adba6cd0f84ee6ab064e7f70cd03a2836cefd2e063fd565180ec13beae93/jsonnet-0.13.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 65.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 25.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.4)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.4)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
            "Requirement already satisfied: PyYAML<=5.1,>=3.10; python_version != \"2.6\" in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Collecting botocore==1.12.176 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/f6/5b67d7c222d6e032e60fd3001db1531c97a39334aa7d6e92d6622bcfa55a/botocore-1.12.176-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Collecting regex (from pytorch-pretrained-bert>=0.6.0->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.0.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.4)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (7.0.4)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Building wheels for collected packages: numpydoc, overrides, word2number, jsonnet, parsimonious, regex\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/30/ab/ae4a57b1df44fa20a531edb9601b27603da8f5336225691f3f\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built numpydoc overrides word2number jsonnet parsimonious regex\n",
            "Installing collected packages: tensorboardX, flaky, numpydoc, rsa, colorama, botocore, awscli, overrides, word2number, conllu, jsonpickle, flask-cors, regex, pytorch-pretrained-bert, unidecode, jsonnet, parsimonious, responses, ftfy, allennlp\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: botocore 1.12.167\n",
            "    Uninstalling botocore-1.12.167:\n",
            "      Successfully uninstalled botocore-1.12.167\n",
            "Successfully installed allennlp-0.8.4 awscli-1.16.186 botocore-1.12.176 colorama-0.3.9 conllu-0.11 flaky-3.6.0 flask-cors-3.0.8 ftfy-5.5.1 jsonnet-0.13.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-1.9 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 responses-0.10.6 rsa-3.4.2 tensorboardX-1.7 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usE8HacRrOfK",
        "colab_type": "text"
      },
      "source": [
        "### Approach\n",
        "\n",
        "We will use mostly the same code as before.  For brevity, I have compacted it all here and omitted parts that arent required for this section.  For more information, see the previous section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN93oU4plXgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import codecs\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, num_classes, embed_dims, rnn_units, rnn_layers=1, dropout=0.5, hidden_units=[]):\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = torch.nn.LSTM(embed_dims,\n",
        "                                 rnn_units,\n",
        "                                 rnn_layers,\n",
        "                                 dropout=dropout,\n",
        "                                 bidirectional=False,\n",
        "                                 batch_first=False)\n",
        "        nn.init.orthogonal_(self.rnn.weight_hh_l0)\n",
        "        nn.init.orthogonal_(self.rnn.weight_ih_l0)\n",
        "        sequence = []\n",
        "        input_units = rnn_units\n",
        "        output_units = rnn_units\n",
        "        for h in hidden_units:\n",
        "            sequence.append(nn.Linear(input_units, h))\n",
        "            input_units = h\n",
        "            output_units = h\n",
        "            \n",
        "        sequence.append(nn.Linear(output_units, num_classes))\n",
        "        self.outputs = nn.Sequential(*sequence)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        one_hots, lengths = inputs\n",
        "        embed = self.dropout(self.embeddings(one_hots))\n",
        "        embed = embed.transpose(0, 1)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.tolist())\n",
        "        _, hidden = self.rnn(packed)\n",
        "        hidden = hidden[0].view(hidden[0].shape[1:])\n",
        "        linear = self.outputs(hidden)\n",
        "        return F.log_softmax(linear, dim=-1)\n",
        "\n",
        "class ConfusionMatrix:\n",
        "    \"\"\"Confusion matrix with metrics\n",
        "\n",
        "    This class accumulates classification output, and tracks it in a confusion matrix.\n",
        "    Metrics are available that use the confusion matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, labels):\n",
        "        \"\"\"Constructor with input labels\n",
        "\n",
        "        :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n",
        "        \"\"\"\n",
        "        if type(labels) is dict:\n",
        "            self.labels = []\n",
        "            for i in range(len(labels)):\n",
        "                self.labels.append(labels[i])\n",
        "        else:\n",
        "            self.labels = labels\n",
        "        nc = len(self.labels)\n",
        "        self._cm = np.zeros((nc, nc), dtype=np.int)\n",
        "\n",
        "    def add(self, truth, guess):\n",
        "        \"\"\"Add a single value to the confusion matrix based off `truth` and `guess`\n",
        "\n",
        "        :param truth: The real `y` value (or ground truth label)\n",
        "        :param guess: The guess for `y` value (or assertion)\n",
        "        \"\"\"\n",
        "\n",
        "        self._cm[truth, guess] += 1\n",
        "\n",
        "    def __str__(self):\n",
        "        values = []\n",
        "        width = max(8, max(len(x) for x in self.labels) + 1)\n",
        "        for i, label in enumerate([''] + self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "        values += ['\\n']\n",
        "        for i, label in enumerate(self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "            for j in range(len(self.labels)):\n",
        "                values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\n",
        "            values += ['\\n']\n",
        "        values += ['\\n']\n",
        "        return ''.join(values)\n",
        "\n",
        "    def save(self, outfile):\n",
        "        ordered_fieldnames = OrderedDict([(\"labels\", None)] + [(l, None) for l in self.labels])\n",
        "        with open(outfile, 'w') as f:\n",
        "            dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)\n",
        "            dw.writeheader()\n",
        "            for index, row in enumerate(self._cm):\n",
        "                row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n",
        "                row_dict.update({\"labels\": self.labels[index]})\n",
        "                dw.writerow(row_dict)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the matrix\n",
        "        \"\"\"\n",
        "        self._cm *= 0\n",
        "\n",
        "    def get_correct(self):\n",
        "        \"\"\"Get the diagonals of the confusion matrix\n",
        "\n",
        "        :return: (``int``) Number of correct classifications\n",
        "        \"\"\"\n",
        "        return self._cm.diagonal().sum()\n",
        "\n",
        "    def get_total(self):\n",
        "        \"\"\"Get total classifications\n",
        "\n",
        "        :return: (``int``) total classifications\n",
        "        \"\"\"\n",
        "        return self._cm.sum()\n",
        "\n",
        "    def get_acc(self):\n",
        "        \"\"\"Get the accuracy\n",
        "\n",
        "        :return: (``float``) accuracy\n",
        "        \"\"\"\n",
        "        return float(self.get_correct())/self.get_total()\n",
        "\n",
        "    def get_recall(self):\n",
        "        \"\"\"Get the recall\n",
        "\n",
        "        :return: (``float``) recall\n",
        "        \"\"\"\n",
        "        total = np.sum(self._cm, axis=1)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_support(self):\n",
        "        return np.sum(self._cm, axis=1)\n",
        "\n",
        "    def get_precision(self):\n",
        "        \"\"\"Get the precision\n",
        "        :return: (``float``) precision\n",
        "        \"\"\"\n",
        "\n",
        "        total = np.sum(self._cm, axis=0)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_mean_precision(self):\n",
        "        \"\"\"Get the mean precision across labels\n",
        "\n",
        "        :return: (``float``) mean precision\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_precision())\n",
        "\n",
        "    def get_weighted_precision(self):\n",
        "        return np.sum(self.get_precision() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_mean_recall(self):\n",
        "        \"\"\"Get the mean recall across labels\n",
        "\n",
        "        :return: (``float``) mean recall\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_recall())\n",
        "\n",
        "    def get_weighted_recall(self):\n",
        "        return np.sum(self.get_recall() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_weighted_f(self, beta=1):\n",
        "        return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_macro_f(self, beta=1):\n",
        "        \"\"\"Get the macro F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) macro F_b\n",
        "        \"\"\"\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        return np.mean(self.get_class_f(beta))\n",
        "\n",
        "    def get_class_f(self, beta=1):\n",
        "        p = self.get_precision()\n",
        "        r = self.get_recall()\n",
        "\n",
        "        b = beta*beta\n",
        "        d = (b * p + r)\n",
        "        d = (d == 0) + d\n",
        "\n",
        "        return (b + 1) * p * r / d\n",
        "\n",
        "    def get_f(self, beta=1):\n",
        "        \"\"\"Get 2 class F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) 2-class F_b\n",
        "        \"\"\"\n",
        "        p = self.get_precision()[1]\n",
        "        r = self.get_recall()[1]\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        d = (beta*beta * p + r)\n",
        "        if d == 0:\n",
        "            return 0\n",
        "        return (beta*beta + 1) * p * r / d\n",
        "\n",
        "    def get_all_metrics(self):\n",
        "        \"\"\"Make a map of metrics suitable for reporting, keyed by metric name\n",
        "\n",
        "        :return: (``dict``) Map of metrics keyed by metric names\n",
        "        \"\"\"\n",
        "        metrics = {'acc': self.get_acc()}\n",
        "        # If 2 class, assume second class is positive AKA 1\n",
        "        if len(self.labels) == 2:\n",
        "            metrics['precision'] = self.get_precision()[1]\n",
        "            metrics['recall'] = self.get_recall()[1]\n",
        "            metrics['f1'] = self.get_f(1)\n",
        "        else:\n",
        "            metrics['mean_precision'] = self.get_mean_precision()\n",
        "            metrics['mean_recall'] = self.get_mean_recall()\n",
        "            metrics['macro_f1'] = self.get_macro_f(1)\n",
        "            metrics['weighted_precision'] = self.get_weighted_precision()\n",
        "            metrics['weighted_recall'] = self.get_weighted_recall()\n",
        "            metrics['weighted_f1'] = self.get_weighted_f(1)\n",
        "        return metrics\n",
        "\n",
        "    def add_batch(self, truth, guess):\n",
        "        \"\"\"Add a batch of data to the confusion matrix\n",
        "\n",
        "        :param truth: The truth tensor\n",
        "        :param guess: The guess tensor\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for truth_i, guess_i in zip(truth, guess):\n",
        "            self.add(truth_i, guess_i)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, optimizer: torch.optim.Optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def run(self, model, labels, train, loss, batch_size): \n",
        "        model.train()       \n",
        "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        cm = ConfusionMatrix(labels)\n",
        "\n",
        "        for batch in train_loader:\n",
        "            loss_value, y_pred, y_actual = self.update(model, loss, batch)\n",
        "            _, best = y_pred.max(1)\n",
        "            yt = y_actual.cpu().int().numpy()\n",
        "            yp = best.cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "\n",
        "        print(cm.get_all_metrics())\n",
        "        return cm\n",
        "    \n",
        "    def update(self, model, loss, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "        x, lengths, y = batch\n",
        "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[perm_idx]\n",
        "        y_sorted = y[perm_idx]\n",
        "        y_sorted = y_sorted.to('cuda:0')\n",
        "        inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "        y_pred = model(inputs)\n",
        "        loss_value = loss(y_pred, y_sorted)\n",
        "        loss_value.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss_value.item(), y_pred, y_sorted\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def run(self, model, labels, dataset, batch_size=1):\n",
        "        model.eval()\n",
        "        valid_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "        cm = ConfusionMatrix(labels)\n",
        "        for batch in valid_loader:\n",
        "            y_pred, y_actual = self.inference(model, batch)\n",
        "            _, best = y_pred.max(1)\n",
        "            yt = y_actual.cpu().int().numpy()\n",
        "            yp = best.cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "        return cm\n",
        "\n",
        "    def inference(self, model, batch):\n",
        "        with torch.no_grad():\n",
        "            x, lengths, y = batch\n",
        "            lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "            x_sorted = x[perm_idx]\n",
        "            y_sorted = y[perm_idx]\n",
        "            y_sorted = y_sorted.to('cuda:0')\n",
        "            inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "            y_pred = model(inputs)\n",
        "            return y_pred, y_sorted\n",
        "\n",
        "def fit(model, labels, optimizer, loss, epochs, batch_size, train, valid, test):\n",
        "\n",
        "    trainer = Trainer(optimizer)\n",
        "    evaluator = Evaluator()\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print('EPOCH {}'.format(epoch + 1))\n",
        "        print('=================================')\n",
        "        print('Training Results')\n",
        "        cm = trainer.run(model, labels, train, loss, batch_size)\n",
        "        print('Validation Results')\n",
        "        cm = evaluator.run(model, labels, valid)\n",
        "        print(cm.get_all_metrics())\n",
        "        if cm.get_acc() > best_acc:\n",
        "            print('New best model {:.2f}'.format(cm.get_acc()))\n",
        "            best_acc = cm.get_acc()\n",
        "            torch.save(model.state_dict(), './checkpoint.pth')\n",
        "    if test:\n",
        "        model.load_state_dict(torch.load('./checkpoint.pth'))\n",
        "        cm = evaluator.run(model, labels, test)\n",
        "        print('Final result')\n",
        "        print(cm.get_all_metrics())\n",
        "    return cm.get_acc()\n",
        "\n",
        "def whitespace_tokenizer(words: str) -> List[str]:\n",
        "    return words.split() \n",
        "\n",
        "def sst2_tokenizer(words: str) -> List[str]:\n",
        "    REPLACE = { \"'s\": \" 's \",\n",
        "                \"'ve\": \" 've \",\n",
        "                \"n't\": \" n't \",\n",
        "                \"'re\": \" 're \",\n",
        "                \"'d\": \" 'd \",\n",
        "                \"'ll\": \" 'll \",\n",
        "                \",\": \" , \",\n",
        "                \"!\": \" ! \",\n",
        "                }\n",
        "    words = words.lower()\n",
        "    words = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", words)\n",
        "    for k, v in REPLACE.items():\n",
        "            words = words.replace(k, v)\n",
        "    return [w.strip() for w in words.split()]\n",
        "\n",
        "\n",
        "class Reader:\n",
        "\n",
        "    def __init__(self, files, lowercase=True, min_freq=0,\n",
        "                 tokenizer=sst2_tokenizer, vectorizer=None):\n",
        "        self.lowercase = lowercase\n",
        "        self.tokenizer = tokenizer\n",
        "        build_vocab = vectorizer is None\n",
        "        self.vectorizer = vectorizer if vectorizer else self._vectorizer\n",
        "        x = Counter()\n",
        "        y = Counter()\n",
        "        for file_name in files:\n",
        "            if file_name is None:\n",
        "                continue\n",
        "            with codecs.open(file_name, encoding='utf-8', mode='r') as f:\n",
        "                for line in f:\n",
        "                    words = line.split()\n",
        "                    y.update(words[0])\n",
        "\n",
        "                    if build_vocab:\n",
        "                        words = self.tokenizer(' '.join(words[1:]))\n",
        "                        words = words if not self.lowercase else [w.lower() for w in words]\n",
        "                        x.update(words)\n",
        "        self.labels = list(y.keys())\n",
        "\n",
        "        if build_vocab:\n",
        "            x = dict(filter(lambda cnt: cnt[1] >= min_freq, x.items()))\n",
        "            alpha = list(x.keys())\n",
        "            alpha.sort()\n",
        "            self.vocab = {w: i+1 for i, w in enumerate(alpha)}\n",
        "            self.vocab['[PAD]'] = 0\n",
        "\n",
        "        self.labels.sort()\n",
        "\n",
        "    def _vectorizer(self, words: List[str]) -> List[int]:\n",
        "        return [self.vocab.get(w, 0) for w in words]\n",
        "\n",
        "    def load(self, filename: str) -> TensorDataset:\n",
        "        label2index = {l: i for i, l in enumerate(self.labels)}\n",
        "        xs = []\n",
        "        lengths = []\n",
        "        ys = []\n",
        "        with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
        "            for line in f:\n",
        "                words = line.split()\n",
        "                ys.append(label2index[words[0]])\n",
        "                words = self.tokenizer(' '.join(words[1:]))\n",
        "                words = words if not self.lowercase else [w.lower() for w in words]\n",
        "                vec = self.vectorizer(words)\n",
        "                lengths.append(len(vec))\n",
        "                xs.append(torch.tensor(vec, dtype=torch.long))\n",
        "        x_tensor = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
        "        lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "        y_tensor = torch.tensor(ys, dtype=torch.long)\n",
        "        return TensorDataset(x_tensor, lengths_tensor, y_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJpvG1NTxSo9",
        "colab_type": "text"
      },
      "source": [
        "### The new thing: set up to use ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2fR03DxeNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "\n",
        "\n",
        "def elmo_vectorizer(sentence):\n",
        "    character_ids = batch_to_ids([sentence])\n",
        "    return character_ids.squeeze(0)\n",
        "\n",
        "  \n",
        "class ElmoEmbedding(nn.Module):\n",
        "    def __init__(self, options_file, weight_file, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.elmo = Elmo(options_file, weight_file, 2, dropout=dropout)\n",
        "    def forward(self, xch):\n",
        "        elmo = self.elmo(xch)\n",
        "        e1, e2 = elmo['elmo_representations']\n",
        "        mask = elmo['mask']\n",
        "        embeddings = (e1 + e2) * mask.float().unsqueeze(-1)\n",
        "        return embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuxm3Ugau6jW",
        "colab_type": "text"
      },
      "source": [
        "As before, we are going to load up our data with a reader.  This time, though, we will provide a vectorizer for ELMo.  In our simple example `Reader`, we only allow a single feature as our input vector to our classifier, so we can stop counting up our vocab.  In real life, you probably want to support both word vector features and context vector features so you might want to modify the code to support both.  This is a very common approach -- just using ELMo to augment an existing setup.  Here, we just look at using ELMo features by themselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NzW34_RwGUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1cbe000c-4bff-499a-bec8-188cd9cceb86"
      },
      "source": [
        "!wget https://www.dropbox.com/s/08km2ean8bkt7p3/trec.tar.gz?dl=1\n",
        "!tar -xzf 'trec.tar.gz?dl=1'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-26 12:27:10--  https://www.dropbox.com/s/08km2ean8bkt7p3/trec.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.1, 2620:100:6031:1::a27d:5101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/08km2ean8bkt7p3/trec.tar.gz [following]\n",
            "--2019-06-26 12:27:11--  https://www.dropbox.com/s/dl/08km2ean8bkt7p3/trec.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com/cd/0/get/AjiDtTF_BsnqtxDcMPgw8EML45xXVCAbpq9FMPNTVGdqlFPIfMP1jWTNxY0pgYwGL4g5yDcpd9shPv7nxmqdzWj7ekJBLzFY8DZkCVe_YQrNPg/file?dl=1# [following]\n",
            "--2019-06-26 12:27:11--  https://uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com/cd/0/get/AjiDtTF_BsnqtxDcMPgw8EML45xXVCAbpq9FMPNTVGdqlFPIfMP1jWTNxY0pgYwGL4g5yDcpd9shPv7nxmqdzWj7ekJBLzFY8DZkCVe_YQrNPg/file?dl=1\n",
            "Resolving uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com (uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com)... 162.125.81.6, 2620:100:6031:6::a27d:5106\n",
            "Connecting to uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com (uc9655c06af3554607eeabe74f7f.dl.dropboxusercontent.com)|162.125.81.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117253 (115K) [application/binary]\n",
            "Saving to: ‘trec.tar.gz?dl=1’\n",
            "\n",
            "trec.tar.gz?dl=1    100%[===================>] 114.50K   580KB/s    in 0.2s    \n",
            "\n",
            "2019-06-26 12:27:12 (580 KB/s) - ‘trec.tar.gz?dl=1’ saved [117253/117253]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPy-A82s048z",
        "colab_type": "text"
      },
      "source": [
        "We will set up our reader slightly differently than in the last experiment.  Here we will use an `elmo_vectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI5r7tmlvgUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "01782253-ad1e-4013-8f96-d8ea64035de6"
      },
      "source": [
        "BASE = 'trec'\n",
        "TRAIN = os.path.join(BASE, 'trec.nodev.utf8')\n",
        "VALID = os.path.join(BASE, 'trec.dev.utf8')\n",
        "TEST = os.path.join(BASE, 'trec.test.utf8')\n",
        "\n",
        "\n",
        "\n",
        "r = Reader((TRAIN, VALID, TEST,), lowercase=False, vectorizer=elmo_vectorizer)\n",
        "train = r.load(TRAIN)\n",
        "valid = r.load(VALID)\n",
        "test = r.load(TEST)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:391: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmE1VJSpws30",
        "colab_type": "text"
      },
      "source": [
        "Building the network is basically the same as before, but we are using ELMo instead of word vectors.  The command below will take a few minutes -- this is a much larger (forward) network than before, even though the learnable parameters havent really changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7y6t65zw_iV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63bb1df7-f562-4478-8a74-39b64ddc00aa"
      },
      "source": [
        "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "embeddings = ElmoEmbedding(options_file, weight_file)\n",
        "model = LSTMClassifier(embeddings, len(r.labels), embed_dims=1024, rnn_units=100, hidden_units=[100])\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model has {num_params} parameters\") \n",
        "\n",
        "\n",
        "model.to('cuda:0')\n",
        "loss = torch.nn.NLLLoss()\n",
        "loss = loss.to('cuda:0')\n",
        "\n",
        "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adadelta(learnable_params, lr=1.0)\n",
        "\n",
        "fit(model, r.labels, optimizer, loss, 12, 50, train, valid, test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model has 461114 parameters\n",
            "EPOCH 1\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.52, 'mean_precision': 0.5709996982500932, 'mean_recall': 0.4607179688562917, 'macro_f1': 0.47822641943140026, 'weighted_precision': 0.5272261561285185, 'weighted_recall': 0.52, 'weighted_f1': 0.5146276073296864}\n",
            "Validation Results\n",
            "{'acc': 0.7787610619469026, 'mean_precision': 0.7408372302325149, 'mean_recall': 0.7422682612647518, 'macro_f1': 0.732661412122254, 'weighted_precision': 0.7809378800113048, 'weighted_recall': 0.7787610619469026, 'weighted_f1': 0.7688030535979937}\n",
            "New best model 0.78\n",
            "EPOCH 2\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8088, 'mean_precision': 0.8157138470419919, 'mean_recall': 0.762659844349094, 'macro_f1': 0.7817567233499173, 'weighted_precision': 0.8096565274550026, 'weighted_recall': 0.8088, 'weighted_f1': 0.808481917833408}\n",
            "Validation Results\n",
            "{'acc': 0.8783185840707964, 'mean_precision': 0.8377755875012601, 'mean_recall': 0.8108450321404167, 'macro_f1': 0.8216311841445192, 'weighted_precision': 0.8768063826973819, 'weighted_recall': 0.8783185840707964, 'weighted_f1': 0.8769906797279005}\n",
            "New best model 0.88\n",
            "EPOCH 3\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8604, 'mean_precision': 0.8685597272186496, 'mean_recall': 0.8278174155911305, 'macro_f1': 0.8445337717711633, 'weighted_precision': 0.8612131640641041, 'weighted_recall': 0.8604, 'weighted_f1': 0.8604181748387495}\n",
            "Validation Results\n",
            "{'acc': 0.8783185840707964, 'mean_precision': 0.8115977362240897, 'mean_recall': 0.8549165782476393, 'macro_f1': 0.8191558142589136, 'weighted_precision': 0.8929100931037321, 'weighted_recall': 0.8783185840707964, 'weighted_f1': 0.881164461708954}\n",
            "EPOCH 4\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8816, 'mean_precision': 0.8827995917499024, 'mean_recall': 0.8500149852536568, 'macro_f1': 0.8640352497303655, 'weighted_precision': 0.8823152409725362, 'weighted_recall': 0.8816, 'weighted_f1': 0.8816806096832961}\n",
            "Validation Results\n",
            "{'acc': 0.8783185840707964, 'mean_precision': 0.8005185367607969, 'mean_recall': 0.8384164178898562, 'macro_f1': 0.8132230043267938, 'weighted_precision': 0.8836412132918544, 'weighted_recall': 0.8783185840707964, 'weighted_f1': 0.8792851741738589}\n",
            "EPOCH 5\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9062, 'mean_precision': 0.9051012827697692, 'mean_recall': 0.8776443163754163, 'macro_f1': 0.8896974251591531, 'weighted_precision': 0.9063422662575993, 'weighted_recall': 0.9062, 'weighted_f1': 0.9060955797256174}\n",
            "Validation Results\n",
            "{'acc': 0.8871681415929203, 'mean_precision': 0.841754600272505, 'mean_recall': 0.8353575725343892, 'macro_f1': 0.8354888618995497, 'weighted_precision': 0.8945360776722479, 'weighted_recall': 0.8871681415929203, 'weighted_f1': 0.8878351487445644}\n",
            "New best model 0.89\n",
            "EPOCH 6\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9128, 'mean_precision': 0.9061419841437853, 'mean_recall': 0.8901375210283534, 'macro_f1': 0.8976328431368089, 'weighted_precision': 0.913097341064057, 'weighted_recall': 0.9128, 'weighted_f1': 0.9128736060132462}\n",
            "Validation Results\n",
            "{'acc': 0.8805309734513275, 'mean_precision': 0.8259071509181476, 'mean_recall': 0.8400312067370089, 'macro_f1': 0.8305658421699825, 'weighted_precision': 0.8848000012238714, 'weighted_recall': 0.8805309734513275, 'weighted_f1': 0.8798787073208549}\n",
            "EPOCH 7\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9172, 'mean_precision': 0.9165939077088766, 'mean_recall': 0.9000940638045628, 'macro_f1': 0.9078019315759228, 'weighted_precision': 0.9176432990037711, 'weighted_recall': 0.9172, 'weighted_f1': 0.917331996063225}\n",
            "Validation Results\n",
            "{'acc': 0.9048672566371682, 'mean_precision': 0.8556168122106896, 'mean_recall': 0.856588782382854, 'macro_f1': 0.8559380107804754, 'weighted_precision': 0.9045314383802061, 'weighted_recall': 0.9048672566371682, 'weighted_f1': 0.9045018869583139}\n",
            "New best model 0.90\n",
            "EPOCH 8\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9252, 'mean_precision': 0.9184060652861129, 'mean_recall': 0.905053044610241, 'macro_f1': 0.9113317273753072, 'weighted_precision': 0.9252956988828368, 'weighted_recall': 0.9252, 'weighted_f1': 0.9251964584401605}\n",
            "Validation Results\n",
            "{'acc': 0.9048672566371682, 'mean_precision': 0.8479237789239334, 'mean_recall': 0.8334993277457802, 'macro_f1': 0.8396062234014189, 'weighted_precision': 0.9054334456421075, 'weighted_recall': 0.9048672566371682, 'weighted_f1': 0.9042344729620383}\n",
            "EPOCH 9\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9306, 'mean_precision': 0.9309662070022298, 'mean_recall': 0.914777640417138, 'macro_f1': 0.9223376836823377, 'weighted_precision': 0.9307243178004675, 'weighted_recall': 0.9306, 'weighted_f1': 0.9305990030313109}\n",
            "Validation Results\n",
            "{'acc': 0.911504424778761, 'mean_precision': 0.8297997003398145, 'mean_recall': 0.8609034712213522, 'macro_f1': 0.8382251703025577, 'weighted_precision': 0.9202256785106907, 'weighted_recall': 0.911504424778761, 'weighted_f1': 0.9145759843371012}\n",
            "New best model 0.91\n",
            "EPOCH 10\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9428, 'mean_precision': 0.9392744551401914, 'mean_recall': 0.9264442957017883, 'macro_f1': 0.9325439415824551, 'weighted_precision': 0.942769154735085, 'weighted_recall': 0.9428, 'weighted_f1': 0.9427476823397094}\n",
            "Validation Results\n",
            "{'acc': 0.9026548672566371, 'mean_precision': 0.8350185777875879, 'mean_recall': 0.853876277776697, 'macro_f1': 0.8410530212166124, 'weighted_precision': 0.9099083009650422, 'weighted_recall': 0.9026548672566371, 'weighted_f1': 0.9049407058182064}\n",
            "EPOCH 11\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.949, 'mean_precision': 0.9488636500113383, 'mean_recall': 0.9397100203343559, 'macro_f1': 0.944124050621243, 'weighted_precision': 0.9490097749218492, 'weighted_recall': 0.949, 'weighted_f1': 0.9489875143957431}\n",
            "Validation Results\n",
            "{'acc': 0.911504424778761, 'mean_precision': 0.8648243236173928, 'mean_recall': 0.8579896783704287, 'macro_f1': 0.8607808657677639, 'weighted_precision': 0.9131900131177974, 'weighted_recall': 0.911504424778761, 'weighted_f1': 0.9116949609607606}\n",
            "EPOCH 12\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9502, 'mean_precision': 0.9461765876247789, 'mean_recall': 0.9421969821932635, 'macro_f1': 0.9441578316389343, 'weighted_precision': 0.9502605644610558, 'weighted_recall': 0.9502, 'weighted_f1': 0.9502228626783008}\n",
            "Validation Results\n",
            "{'acc': 0.9004424778761062, 'mean_precision': 0.8414586426400703, 'mean_recall': 0.8523499671621169, 'macro_f1': 0.8460939393179038, 'weighted_precision': 0.9022638522354115, 'weighted_recall': 0.9004424778761062, 'weighted_f1': 0.9008417439792548}\n",
            "Final result\n",
            "{'acc': 0.946, 'mean_precision': 0.9153662108912811, 'mean_recall': 0.9365949747031941, 'macro_f1': 0.9236279306903604, 'weighted_precision': 0.9475955878488813, 'weighted_recall': 0.946, 'weighted_f1': 0.945494404550159}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgrkf2EP4htl",
        "colab_type": "text"
      },
      "source": [
        "Let's see how this number compares against a randomly initialized baseline model that is otherwise identical.  We dont really need to use such a huge embedding size in this case -- we are using word vectors instead of character compositional vectors and we dont really have enough information to train a huge word embedding from scratch.  Also, since we dont have much information, we will use lowercased features.  Note that using these word embeddings features, our model has **6x more parameters than before**.  Also, we might want to train it longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b97zMOBr3-UA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7833acb5-34c7-478b-f87f-f4efcfe76826"
      },
      "source": [
        "\n",
        "r = Reader((TRAIN, VALID, TEST,), lowercase=True)\n",
        "train = r.load(TRAIN)\n",
        "valid = r.load(VALID)\n",
        "test = r.load(TEST)\n",
        "\n",
        "embeddings = nn.Embedding(len(r.vocab), 300)\n",
        "model = LSTMClassifier(embeddings, len(r.labels), embeddings.weight.shape[1], rnn_units=100, hidden_units=[100])\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model has {num_params} parameters\") \n",
        "\n",
        "\n",
        "model.to('cuda:0')\n",
        "loss = torch.nn.NLLLoss()\n",
        "loss = loss.to('cuda:0')\n",
        "\n",
        "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adadelta(learnable_params, lr=1.0)\n",
        "\n",
        "fit(model, r.labels, optimizer, loss, 48, 50, train, valid, test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model has 2801306 parameters\n",
            "EPOCH 1\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.2542, 'mean_precision': 0.2711038695766469, 'mean_recall': 0.22218244347567548, 'macro_f1': 0.21425818700631108, 'weighted_precision': 0.2564855224194423, 'weighted_recall': 0.2542, 'weighted_f1': 0.22703312209459242}\n",
            "Validation Results\n",
            "{'acc': 0.3163716814159292, 'mean_precision': 0.38163668256669087, 'mean_recall': 0.2838794512309889, 'macro_f1': 0.254044176761413, 'weighted_precision': 0.3912014117742859, 'weighted_recall': 0.3163716814159292, 'weighted_f1': 0.2676486277978532}\n",
            "New best model 0.32\n",
            "EPOCH 2\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.3816, 'mean_precision': 0.4291024620399562, 'mean_recall': 0.3640440077585961, 'macro_f1': 0.3785247578651236, 'weighted_precision': 0.3791969668224335, 'weighted_recall': 0.3816, 'weighted_f1': 0.3700027369634422}\n",
            "Validation Results\n",
            "{'acc': 0.5154867256637168, 'mean_precision': 0.4800233611955989, 'mean_recall': 0.46191577496493413, 'macro_f1': 0.44646430059374564, 'weighted_precision': 0.5409651661696213, 'weighted_recall': 0.5154867256637168, 'weighted_f1': 0.4990796161295456}\n",
            "New best model 0.52\n",
            "EPOCH 3\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.541, 'mean_precision': 0.5869750112343551, 'mean_recall': 0.5112143243214295, 'macro_f1': 0.5333602972579561, 'weighted_precision': 0.5390816470442898, 'weighted_recall': 0.541, 'weighted_f1': 0.5373228130274065}\n",
            "Validation Results\n",
            "{'acc': 0.665929203539823, 'mean_precision': 0.6502458686014166, 'mean_recall': 0.611644345674543, 'macro_f1': 0.6253154467809023, 'weighted_precision': 0.6689423955079472, 'weighted_recall': 0.665929203539823, 'weighted_f1': 0.6661037015959343}\n",
            "New best model 0.67\n",
            "EPOCH 4\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.6616, 'mean_precision': 0.6856923728680427, 'mean_recall': 0.6262266690699423, 'macro_f1': 0.6475140288478585, 'weighted_precision': 0.6655008067141199, 'weighted_recall': 0.6616, 'weighted_f1': 0.6623984046600156}\n",
            "Validation Results\n",
            "{'acc': 0.7323008849557522, 'mean_precision': 0.7169981326168285, 'mean_recall': 0.6666702933588001, 'macro_f1': 0.6840723235874991, 'weighted_precision': 0.7464716844408581, 'weighted_recall': 0.7323008849557522, 'weighted_f1': 0.7356034641302298}\n",
            "New best model 0.73\n",
            "EPOCH 5\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.7288, 'mean_precision': 0.7540145928742303, 'mean_recall': 0.6910001925865, 'macro_f1': 0.7139437407202665, 'weighted_precision': 0.7343307916690934, 'weighted_recall': 0.7288, 'weighted_f1': 0.7302264126848796}\n",
            "Validation Results\n",
            "{'acc': 0.7411504424778761, 'mean_precision': 0.7566390868366094, 'mean_recall': 0.692456910889724, 'macro_f1': 0.7167896240590302, 'weighted_precision': 0.7686154262617612, 'weighted_recall': 0.7411504424778761, 'weighted_f1': 0.7482032389633049}\n",
            "New best model 0.74\n",
            "EPOCH 6\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.7716, 'mean_precision': 0.8023505897555827, 'mean_recall': 0.7450176656686777, 'macro_f1': 0.7677225007190788, 'weighted_precision': 0.7786197450638762, 'weighted_recall': 0.7716, 'weighted_f1': 0.7736909572704377}\n",
            "Validation Results\n",
            "{'acc': 0.7699115044247787, 'mean_precision': 0.7681434368163244, 'mean_recall': 0.720845466736241, 'macro_f1': 0.7396121686086982, 'weighted_precision': 0.7852490818216408, 'weighted_recall': 0.7699115044247787, 'weighted_f1': 0.7742172596581404}\n",
            "New best model 0.77\n",
            "EPOCH 7\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8062, 'mean_precision': 0.8280017479075008, 'mean_recall': 0.7686857165107716, 'macro_f1': 0.7913675571636053, 'weighted_precision': 0.8108177810797202, 'weighted_recall': 0.8062, 'weighted_f1': 0.8073339694803928}\n",
            "Validation Results\n",
            "{'acc': 0.7654867256637168, 'mean_precision': 0.7281848255703753, 'mean_recall': 0.7189770508143988, 'macro_f1': 0.7192443636506254, 'weighted_precision': 0.7834946227945238, 'weighted_recall': 0.7654867256637168, 'weighted_f1': 0.7695609410429685}\n",
            "EPOCH 8\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8188, 'mean_precision': 0.8331938333742953, 'mean_recall': 0.7777355933214395, 'macro_f1': 0.7991448859501534, 'weighted_precision': 0.8234884472766706, 'weighted_recall': 0.8188, 'weighted_f1': 0.8199174837688515}\n",
            "Validation Results\n",
            "{'acc': 0.7876106194690266, 'mean_precision': 0.7406712840686446, 'mean_recall': 0.741177387490446, 'macro_f1': 0.7391340607348322, 'weighted_precision': 0.7934456918988625, 'weighted_recall': 0.7876106194690266, 'weighted_f1': 0.7885437464981077}\n",
            "New best model 0.79\n",
            "EPOCH 9\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8438, 'mean_precision': 0.8580430238941142, 'mean_recall': 0.82496170171752, 'macro_f1': 0.8394009688377225, 'weighted_precision': 0.8461362415877481, 'weighted_recall': 0.8438, 'weighted_f1': 0.8445461747588201}\n",
            "Validation Results\n",
            "{'acc': 0.8008849557522124, 'mean_precision': 0.7722468757786594, 'mean_recall': 0.7484462015183371, 'macro_f1': 0.7587037804126058, 'weighted_precision': 0.8098727472199789, 'weighted_recall': 0.8008849557522124, 'weighted_f1': 0.8038320572857367}\n",
            "New best model 0.80\n",
            "EPOCH 10\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8624, 'mean_precision': 0.8672500115041349, 'mean_recall': 0.840099803906873, 'macro_f1': 0.8521938882647486, 'weighted_precision': 0.864914139249267, 'weighted_recall': 0.8624, 'weighted_f1': 0.8631911620473937}\n",
            "Validation Results\n",
            "{'acc': 0.8185840707964602, 'mean_precision': 0.7928570323516021, 'mean_recall': 0.7643570297177074, 'macro_f1': 0.7760497481469945, 'weighted_precision': 0.8198282908188204, 'weighted_recall': 0.8185840707964602, 'weighted_f1': 0.8187582685131565}\n",
            "New best model 0.82\n",
            "EPOCH 11\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8692, 'mean_precision': 0.8796289784822008, 'mean_recall': 0.848559460102931, 'macro_f1': 0.8623826086440552, 'weighted_precision': 0.8711983021975033, 'weighted_recall': 0.8692, 'weighted_f1': 0.8697855168183398}\n",
            "Validation Results\n",
            "{'acc': 0.8185840707964602, 'mean_precision': 0.7934518827573193, 'mean_recall': 0.7651111767407278, 'macro_f1': 0.7761656804207431, 'weighted_precision': 0.8214175254158063, 'weighted_recall': 0.8185840707964602, 'weighted_f1': 0.8188705166117247}\n",
            "EPOCH 12\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8846, 'mean_precision': 0.8875722693705613, 'mean_recall': 0.8665106315614238, 'macro_f1': 0.8762177244854698, 'weighted_precision': 0.8854841747026189, 'weighted_recall': 0.8846, 'weighted_f1': 0.8848854346065691}\n",
            "Validation Results\n",
            "{'acc': 0.8252212389380531, 'mean_precision': 0.8005742240560346, 'mean_recall': 0.7686886178181688, 'macro_f1': 0.7812123089713506, 'weighted_precision': 0.83027661290786, 'weighted_recall': 0.8252212389380531, 'weighted_f1': 0.8261536005089094}\n",
            "New best model 0.83\n",
            "EPOCH 13\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8906, 'mean_precision': 0.9024596558649328, 'mean_recall': 0.8683807160434519, 'macro_f1': 0.8830477476882045, 'weighted_precision': 0.8916850371274686, 'weighted_recall': 0.8906, 'weighted_f1': 0.890801658782816}\n",
            "Validation Results\n",
            "{'acc': 0.8252212389380531, 'mean_precision': 0.7812130248454939, 'mean_recall': 0.7896516300640677, 'macro_f1': 0.7825709138241864, 'weighted_precision': 0.8335139148714263, 'weighted_recall': 0.8252212389380531, 'weighted_f1': 0.8273127484928323}\n",
            "EPOCH 14\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8996, 'mean_precision': 0.9033429444158068, 'mean_recall': 0.8898124562283561, 'macro_f1': 0.8962389347347548, 'weighted_precision': 0.900772893670345, 'weighted_recall': 0.8996, 'weighted_f1': 0.9000223616958487}\n",
            "Validation Results\n",
            "{'acc': 0.8207964601769911, 'mean_precision': 0.7855504319986605, 'mean_recall': 0.7626725002943253, 'macro_f1': 0.7724138141128338, 'weighted_precision': 0.8288197568699547, 'weighted_recall': 0.8207964601769911, 'weighted_f1': 0.823138690833502}\n",
            "EPOCH 15\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9086, 'mean_precision': 0.9158820508119687, 'mean_recall': 0.892509601283353, 'macro_f1': 0.9031282134549166, 'weighted_precision': 0.909610117829027, 'weighted_recall': 0.9086, 'weighted_f1': 0.9088725495218385}\n",
            "Validation Results\n",
            "{'acc': 0.8163716814159292, 'mean_precision': 0.7722122820420035, 'mean_recall': 0.7796354851662128, 'macro_f1': 0.7731907492973797, 'weighted_precision': 0.8258425772956993, 'weighted_recall': 0.8163716814159292, 'weighted_f1': 0.8191054530963932}\n",
            "EPOCH 16\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9122, 'mean_precision': 0.9155402638139463, 'mean_recall': 0.8933551531588098, 'macro_f1': 0.9034836853339239, 'weighted_precision': 0.9125089623653919, 'weighted_recall': 0.9122, 'weighted_f1': 0.9122429250648686}\n",
            "Validation Results\n",
            "{'acc': 0.8230088495575221, 'mean_precision': 0.7853784675307006, 'mean_recall': 0.7878415934108253, 'macro_f1': 0.7850911405112271, 'weighted_precision': 0.8295871200994618, 'weighted_recall': 0.8230088495575221, 'weighted_f1': 0.824742421870456}\n",
            "EPOCH 17\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.912, 'mean_precision': 0.9160595271228433, 'mean_recall': 0.8949512047447222, 'macro_f1': 0.9046734963325398, 'weighted_precision': 0.9125359437222418, 'weighted_recall': 0.912, 'weighted_f1': 0.9121414564354678}\n",
            "Validation Results\n",
            "{'acc': 0.8185840707964602, 'mean_precision': 0.7908407140485086, 'mean_recall': 0.7621248105770654, 'macro_f1': 0.7733454527321677, 'weighted_precision': 0.8211218482589827, 'weighted_recall': 0.8185840707964602, 'weighted_f1': 0.8186201866270197}\n",
            "EPOCH 18\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9208, 'mean_precision': 0.920860160162642, 'mean_recall': 0.9080707501318526, 'macro_f1': 0.9141405920038642, 'weighted_precision': 0.9210455704120348, 'weighted_recall': 0.9208, 'weighted_f1': 0.9208724249373011}\n",
            "Validation Results\n",
            "{'acc': 0.834070796460177, 'mean_precision': 0.7847310823035486, 'mean_recall': 0.799248053281533, 'macro_f1': 0.7912491651770943, 'weighted_precision': 0.8347008642835921, 'weighted_recall': 0.834070796460177, 'weighted_f1': 0.8339746335395282}\n",
            "New best model 0.83\n",
            "EPOCH 19\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.923, 'mean_precision': 0.9240986480660585, 'mean_recall': 0.9208617454729828, 'macro_f1': 0.9224365971080619, 'weighted_precision': 0.9234308480150505, 'weighted_recall': 0.923, 'weighted_f1': 0.9231690166860861}\n",
            "Validation Results\n",
            "{'acc': 0.8407079646017699, 'mean_precision': 0.8147005537125883, 'mean_recall': 0.7789721527002494, 'macro_f1': 0.7933355428994053, 'weighted_precision': 0.845820782786216, 'weighted_recall': 0.8407079646017699, 'weighted_f1': 0.8415883161838554}\n",
            "New best model 0.84\n",
            "EPOCH 20\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9322, 'mean_precision': 0.9370656198707813, 'mean_recall': 0.9250382615603122, 'macro_f1': 0.930799435005969, 'weighted_precision': 0.9324964329780009, 'weighted_recall': 0.9322, 'weighted_f1': 0.9323048507026273}\n",
            "Validation Results\n",
            "{'acc': 0.8429203539823009, 'mean_precision': 0.807537168093869, 'mean_recall': 0.7854655347112298, 'macro_f1': 0.7938398553715454, 'weighted_precision': 0.8408715522668716, 'weighted_recall': 0.8429203539823009, 'weighted_f1': 0.8413402442108973}\n",
            "New best model 0.84\n",
            "EPOCH 21\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9392, 'mean_precision': 0.944783580619183, 'mean_recall': 0.9344823250881666, 'macro_f1': 0.9394544449921841, 'weighted_precision': 0.939384613730245, 'weighted_recall': 0.9392, 'weighted_f1': 0.9392613836123542}\n",
            "Validation Results\n",
            "{'acc': 0.838495575221239, 'mean_precision': 0.7830317830317829, 'mean_recall': 0.7811827756406861, 'macro_f1': 0.7807242844962577, 'weighted_precision': 0.8440288407102567, 'weighted_recall': 0.838495575221239, 'weighted_f1': 0.8396717903981084}\n",
            "EPOCH 22\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.933, 'mean_precision': 0.9340916212391969, 'mean_recall': 0.9310127242439862, 'macro_f1': 0.9325106725967686, 'weighted_precision': 0.9333498748119091, 'weighted_recall': 0.933, 'weighted_f1': 0.9331306222899549}\n",
            "Validation Results\n",
            "{'acc': 0.8296460176991151, 'mean_precision': 0.8041635138073495, 'mean_recall': 0.7709866912512976, 'macro_f1': 0.7832133339189117, 'weighted_precision': 0.836549261382332, 'weighted_recall': 0.8296460176991151, 'weighted_f1': 0.8303778192670614}\n",
            "EPOCH 23\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9358, 'mean_precision': 0.937540039357908, 'mean_recall': 0.9281766297412156, 'macro_f1': 0.9326746791590687, 'weighted_precision': 0.9360526019306357, 'weighted_recall': 0.9358, 'weighted_f1': 0.9358799053692554}\n",
            "Validation Results\n",
            "{'acc': 0.831858407079646, 'mean_precision': 0.7735794299255039, 'mean_recall': 0.7960452423665068, 'macro_f1': 0.781500914664826, 'weighted_precision': 0.8376690138206604, 'weighted_recall': 0.831858407079646, 'weighted_f1': 0.8335152238400367}\n",
            "EPOCH 24\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9428, 'mean_precision': 0.939908501896137, 'mean_recall': 0.9355375879134025, 'macro_f1': 0.937663594989686, 'weighted_precision': 0.9431034876688147, 'weighted_recall': 0.9428, 'weighted_f1': 0.9429043563704896}\n",
            "Validation Results\n",
            "{'acc': 0.8407079646017699, 'mean_precision': 0.7757729859149788, 'mean_recall': 0.7808429064646556, 'macro_f1': 0.776970188313299, 'weighted_precision': 0.845893562502929, 'weighted_recall': 0.8407079646017699, 'weighted_f1': 0.8419676786436744}\n",
            "EPOCH 25\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9438, 'mean_precision': 0.9427555357167029, 'mean_recall': 0.9344010193543021, 'macro_f1': 0.9384483941090781, 'weighted_precision': 0.9440856936672309, 'weighted_recall': 0.9438, 'weighted_f1': 0.943896262862946}\n",
            "Validation Results\n",
            "{'acc': 0.8539823008849557, 'mean_precision': 0.8177785746225817, 'mean_recall': 0.7922341779490948, 'macro_f1': 0.8022164918747596, 'weighted_precision': 0.8549948962634585, 'weighted_recall': 0.8539823008849557, 'weighted_f1': 0.8538506970894914}\n",
            "New best model 0.85\n",
            "EPOCH 26\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9466, 'mean_precision': 0.9476373581723743, 'mean_recall': 0.9467748591779325, 'macro_f1': 0.9471846170471189, 'weighted_precision': 0.9468653126928724, 'weighted_recall': 0.9466, 'weighted_f1': 0.9467046462299026}\n",
            "Validation Results\n",
            "{'acc': 0.8495575221238938, 'mean_precision': 0.7892862056167451, 'mean_recall': 0.7887244553658733, 'macro_f1': 0.7880517471149472, 'weighted_precision': 0.8530878835821191, 'weighted_recall': 0.8495575221238938, 'weighted_f1': 0.8502479311997405}\n",
            "EPOCH 27\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9548, 'mean_precision': 0.950451712216903, 'mean_recall': 0.9492162140742589, 'macro_f1': 0.9498030913571514, 'weighted_precision': 0.9550569029523738, 'weighted_recall': 0.9548, 'weighted_f1': 0.954889899464727}\n",
            "Validation Results\n",
            "{'acc': 0.838495575221239, 'mean_precision': 0.7850825887789532, 'mean_recall': 0.7772311082100002, 'macro_f1': 0.7774689684769508, 'weighted_precision': 0.8527080926674292, 'weighted_recall': 0.838495575221239, 'weighted_f1': 0.841060055278181}\n",
            "EPOCH 28\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9412, 'mean_precision': 0.9367285277532261, 'mean_recall': 0.9345582371176362, 'macro_f1': 0.9356278118025894, 'weighted_precision': 0.9413305188919563, 'weighted_recall': 0.9412, 'weighted_f1': 0.941253849766645}\n",
            "Validation Results\n",
            "{'acc': 0.8473451327433629, 'mean_precision': 0.7833602391211087, 'mean_recall': 0.7873162467889276, 'macro_f1': 0.7834928126138164, 'weighted_precision': 0.8537954919548801, 'weighted_recall': 0.8473451327433629, 'weighted_f1': 0.8487625360316815}\n",
            "EPOCH 29\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9496, 'mean_precision': 0.9446213581275615, 'mean_recall': 0.937888814588693, 'macro_f1': 0.9411509210150295, 'weighted_precision': 0.9496058592571911, 'weighted_recall': 0.9496, 'weighted_f1': 0.9495877389969466}\n",
            "Validation Results\n",
            "{'acc': 0.8473451327433629, 'mean_precision': 0.7937596407611931, 'mean_recall': 0.8068119227760443, 'macro_f1': 0.7979386283840277, 'weighted_precision': 0.8534174535724667, 'weighted_recall': 0.8473451327433629, 'weighted_f1': 0.8489448923181132}\n",
            "EPOCH 30\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9522, 'mean_precision': 0.955389544578479, 'mean_recall': 0.9434051186861612, 'macro_f1': 0.9491273767268043, 'weighted_precision': 0.9524502164490392, 'weighted_recall': 0.9522, 'weighted_f1': 0.9522624454257282}\n",
            "Validation Results\n",
            "{'acc': 0.8429203539823009, 'mean_precision': 0.7954600632555208, 'mean_recall': 0.8060139897834944, 'macro_f1': 0.7995829164772844, 'weighted_precision': 0.8462361310316633, 'weighted_recall': 0.8429203539823009, 'weighted_f1': 0.8436403840634059}\n",
            "EPOCH 31\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9558, 'mean_precision': 0.9558542877675529, 'mean_recall': 0.9479179252469047, 'macro_f1': 0.9517729709816006, 'weighted_precision': 0.9559073665675207, 'weighted_recall': 0.9558, 'weighted_f1': 0.9558315359107069}\n",
            "Validation Results\n",
            "{'acc': 0.8539823008849557, 'mean_precision': 0.8035152469078933, 'mean_recall': 0.7922868659373229, 'macro_f1': 0.7971610373715167, 'weighted_precision': 0.8547072391016901, 'weighted_recall': 0.8539823008849557, 'weighted_f1': 0.8539508302435461}\n",
            "EPOCH 32\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9542, 'mean_precision': 0.9490881924381797, 'mean_recall': 0.9470609294324411, 'macro_f1': 0.9480659757917141, 'weighted_precision': 0.9542463827156913, 'weighted_recall': 0.9542, 'weighted_f1': 0.954219369261186}\n",
            "Validation Results\n",
            "{'acc': 0.8407079646017699, 'mean_precision': 0.779173701745521, 'mean_recall': 0.8056748690207005, 'macro_f1': 0.7889921710806548, 'weighted_precision': 0.8449910120403322, 'weighted_recall': 0.8407079646017699, 'weighted_f1': 0.8415183375195532}\n",
            "EPOCH 33\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9544, 'mean_precision': 0.954788259307079, 'mean_recall': 0.9510876802718863, 'macro_f1': 0.9528856494500942, 'weighted_precision': 0.9545099896553573, 'weighted_recall': 0.9544, 'weighted_f1': 0.9544166176099685}\n",
            "Validation Results\n",
            "{'acc': 0.8451327433628318, 'mean_precision': 0.7945915443622913, 'mean_recall': 0.7856696541744261, 'macro_f1': 0.788517640258899, 'weighted_precision': 0.8453437537481525, 'weighted_recall': 0.8451327433628318, 'weighted_f1': 0.8437976419463735}\n",
            "EPOCH 34\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9578, 'mean_precision': 0.9534951614884588, 'mean_recall': 0.9495933887885218, 'macro_f1': 0.9515141185229957, 'weighted_precision': 0.9578576497163398, 'weighted_recall': 0.9578, 'weighted_f1': 0.9578203388824131}\n",
            "Validation Results\n",
            "{'acc': 0.834070796460177, 'mean_precision': 0.7860273961197959, 'mean_recall': 0.7765317972578236, 'macro_f1': 0.7802502924833106, 'weighted_precision': 0.8347018490079233, 'weighted_recall': 0.834070796460177, 'weighted_f1': 0.8336262437197614}\n",
            "EPOCH 35\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9594, 'mean_precision': 0.9507461520764803, 'mean_recall': 0.9472529226340337, 'macro_f1': 0.948973983632953, 'weighted_precision': 0.9594058718257905, 'weighted_recall': 0.9594, 'weighted_f1': 0.9593990603305516}\n",
            "Validation Results\n",
            "{'acc': 0.8429203539823009, 'mean_precision': 0.7978317507681809, 'mean_recall': 0.8041657051150564, 'macro_f1': 0.7999353888685826, 'weighted_precision': 0.8458719389439029, 'weighted_recall': 0.8429203539823009, 'weighted_f1': 0.8435814074880265}\n",
            "EPOCH 36\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9586, 'mean_precision': 0.9517803035130984, 'mean_recall': 0.9549323292120285, 'macro_f1': 0.9533259088391312, 'weighted_precision': 0.9587087534701512, 'weighted_recall': 0.9586, 'weighted_f1': 0.958643462115947}\n",
            "Validation Results\n",
            "{'acc': 0.8407079646017699, 'mean_precision': 0.8106159671868127, 'mean_recall': 0.8021276149558481, 'macro_f1': 0.8047384535843443, 'weighted_precision': 0.847370195804343, 'weighted_recall': 0.8407079646017699, 'weighted_f1': 0.8419445108206383}\n",
            "EPOCH 37\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9594, 'mean_precision': 0.9594956243204127, 'mean_recall': 0.9573341390574054, 'macro_f1': 0.9583948071101401, 'weighted_precision': 0.9595281477131724, 'weighted_recall': 0.9594, 'weighted_f1': 0.9594452938186878}\n",
            "Validation Results\n",
            "{'acc': 0.8473451327433629, 'mean_precision': 0.8195529747276593, 'mean_recall': 0.829514859169457, 'macro_f1': 0.8232403272387909, 'weighted_precision': 0.8512080403098858, 'weighted_recall': 0.8473451327433629, 'weighted_f1': 0.8482689561206019}\n",
            "EPOCH 38\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9588, 'mean_precision': 0.9623387707805988, 'mean_recall': 0.95238272036402, 'macro_f1': 0.9571835827650349, 'weighted_precision': 0.9589138490493355, 'weighted_recall': 0.9588, 'weighted_f1': 0.9588289775709069}\n",
            "Validation Results\n",
            "{'acc': 0.8539823008849557, 'mean_precision': 0.8082267491877061, 'mean_recall': 0.8340810649520188, 'macro_f1': 0.8165000162450687, 'weighted_precision': 0.8605143793763707, 'weighted_recall': 0.8539823008849557, 'weighted_f1': 0.8549970049598762}\n",
            "EPOCH 39\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9654, 'mean_precision': 0.9621306634277023, 'mean_recall': 0.9641786358890935, 'macro_f1': 0.96313773835027, 'weighted_precision': 0.9654736145947307, 'weighted_recall': 0.9654, 'weighted_f1': 0.9654224831923118}\n",
            "Validation Results\n",
            "{'acc': 0.8407079646017699, 'mean_precision': 0.8004941533355213, 'mean_recall': 0.8047366123415207, 'macro_f1': 0.8022598497890675, 'weighted_precision': 0.8399385088871232, 'weighted_recall': 0.8407079646017699, 'weighted_f1': 0.8398901024740366}\n",
            "EPOCH 40\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9636, 'mean_precision': 0.9588330970757872, 'mean_recall': 0.9623076103629465, 'macro_f1': 0.9605447212200416, 'weighted_precision': 0.9636385833365537, 'weighted_recall': 0.9636, 'weighted_f1': 0.9636155914503978}\n",
            "Validation Results\n",
            "{'acc': 0.8495575221238938, 'mean_precision': 0.8022466268888153, 'mean_recall': 0.809510113409775, 'macro_f1': 0.8046742284956111, 'weighted_precision': 0.8530776681583694, 'weighted_recall': 0.8495575221238938, 'weighted_f1': 0.8502507963485176}\n",
            "EPOCH 41\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9664, 'mean_precision': 0.9688694955903996, 'mean_recall': 0.9666337415562204, 'macro_f1': 0.9677387029307273, 'weighted_precision': 0.9664840549413275, 'weighted_recall': 0.9664, 'weighted_f1': 0.9664329137854561}\n",
            "Validation Results\n",
            "{'acc': 0.8495575221238938, 'mean_precision': 0.8111041600242189, 'mean_recall': 0.8088464333035863, 'macro_f1': 0.809513544989108, 'weighted_precision': 0.8519585768734702, 'weighted_recall': 0.8495575221238938, 'weighted_f1': 0.8501414337451356}\n",
            "EPOCH 42\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9676, 'mean_precision': 0.9637965879802303, 'mean_recall': 0.9596855623071251, 'macro_f1': 0.9617099158220125, 'weighted_precision': 0.9676610331582651, 'weighted_recall': 0.9676, 'weighted_f1': 0.9676206084290838}\n",
            "Validation Results\n",
            "{'acc': 0.8495575221238938, 'mean_precision': 0.8119965092213777, 'mean_recall': 0.8084992834178563, 'macro_f1': 0.8095770767961268, 'weighted_precision': 0.8525445886796569, 'weighted_recall': 0.8495575221238938, 'weighted_f1': 0.8501438295199143}\n",
            "EPOCH 43\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.968, 'mean_precision': 0.9634052633099465, 'mean_recall': 0.9559416264865014, 'macro_f1': 0.959567271968569, 'weighted_precision': 0.9680246079414363, 'weighted_recall': 0.968, 'weighted_f1': 0.9679978117464867}\n",
            "Validation Results\n",
            "{'acc': 0.8584070796460177, 'mean_precision': 0.8193689008373114, 'mean_recall': 0.8158853615034496, 'macro_f1': 0.8162194963390778, 'weighted_precision': 0.8617681133120891, 'weighted_recall': 0.8584070796460177, 'weighted_f1': 0.8583128669798775}\n",
            "New best model 0.86\n",
            "EPOCH 44\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9666, 'mean_precision': 0.9616576221786529, 'mean_recall': 0.9607774158901504, 'macro_f1': 0.9612070053336597, 'weighted_precision': 0.9667073353291762, 'weighted_recall': 0.9666, 'weighted_f1': 0.9666417545751771}\n",
            "Validation Results\n",
            "{'acc': 0.8561946902654868, 'mean_precision': 0.8163633186887043, 'mean_recall': 0.813159131939746, 'macro_f1': 0.8112590463405476, 'weighted_precision': 0.867918677613742, 'weighted_recall': 0.8561946902654868, 'weighted_f1': 0.8578443366649067}\n",
            "EPOCH 45\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.962, 'mean_precision': 0.9574423336624155, 'mean_recall': 0.955102238018331, 'macro_f1': 0.9562584431220277, 'weighted_precision': 0.9620641135237417, 'weighted_recall': 0.962, 'weighted_f1': 0.962023032869185}\n",
            "Validation Results\n",
            "{'acc': 0.8517699115044248, 'mean_precision': 0.8117385643928597, 'mean_recall': 0.7868983604667591, 'macro_f1': 0.7964669080053696, 'weighted_precision': 0.8598782319694347, 'weighted_recall': 0.8517699115044248, 'weighted_f1': 0.8526380933290394}\n",
            "EPOCH 46\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9648, 'mean_precision': 0.9607092130222116, 'mean_recall': 0.9533622956249764, 'macro_f1': 0.9569287714328466, 'weighted_precision': 0.9648080653540401, 'weighted_recall': 0.9648, 'weighted_f1': 0.9647883002669175}\n",
            "Validation Results\n",
            "{'acc': 0.8561946902654868, 'mean_precision': 0.8217930397001066, 'mean_recall': 0.8362654858387829, 'macro_f1': 0.8252522180441889, 'weighted_precision': 0.8649103767808297, 'weighted_recall': 0.8561946902654868, 'weighted_f1': 0.8575408377750767}\n",
            "EPOCH 47\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9696, 'mean_precision': 0.9670533594725588, 'mean_recall': 0.963246533119368, 'macro_f1': 0.9651152695349783, 'weighted_precision': 0.9696296268094775, 'weighted_recall': 0.9696, 'weighted_f1': 0.969602370154023}\n",
            "Validation Results\n",
            "{'acc': 0.8539823008849557, 'mean_precision': 0.813953901662522, 'mean_recall': 0.837905480879399, 'macro_f1': 0.8219907543436955, 'weighted_precision': 0.8601571068633407, 'weighted_recall': 0.8539823008849557, 'weighted_f1': 0.8540495282524173}\n",
            "EPOCH 48\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9716, 'mean_precision': 0.9729301129887138, 'mean_recall': 0.9694122545751984, 'macro_f1': 0.9711431833045272, 'weighted_precision': 0.9715825141210201, 'weighted_recall': 0.9716, 'weighted_f1': 0.9715867692654346}\n",
            "Validation Results\n",
            "{'acc': 0.8628318584070797, 'mean_precision': 0.8279838520337671, 'mean_recall': 0.8165713528253296, 'macro_f1': 0.8198032761873382, 'weighted_precision': 0.8705915832093555, 'weighted_recall': 0.8628318584070797, 'weighted_f1': 0.8633846973507786}\n",
            "New best model 0.86\n",
            "Final result\n",
            "{'acc': 0.882, 'mean_precision': 0.881767450694663, 'mean_recall': 0.8604990891409473, 'macro_f1': 0.8699204254533504, 'weighted_precision': 0.8849781116608199, 'weighted_recall': 0.882, 'weighted_f1': 0.8823807050276581}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPWVd-iW1mMG",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Without even concatenating word features, our ELMo model, with far fewer parameters, surpasses the performance of the randomly initialized baseline, which we would expect.  It also significantly out-performs our CNN pre-trained, fine-tuned word embeddings baseline from the last section -- that model's max performance is around 93.  Note that this dataset is tiny, and the variance is large between datasets, but this model consistently outperforms both CNN and LSTM baselines.\n",
        "\n",
        "Contextual embeddings consistently outperform non-contextual embeddings on almost every task in NLP, not just in text classification.  This method is becoming so commonly used that some papers have even started reporting this approach as a baseline.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}