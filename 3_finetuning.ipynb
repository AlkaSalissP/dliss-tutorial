{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-finetuning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/3_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DogOyWz4J0Oj",
        "colab_type": "text"
      },
      "source": [
        "# Part III: Fine-tuning a pre-trained model\n",
        "\n",
        "In the last section, we looked at using a biLM networks layers as embeddings for our classification model.  In that approach, we maintain the exact same model architecture as before, but just switching our word embeddings out for context embeddings (or, more commonly, using them in concert).\n",
        "\n",
        "The paper [Improving Language Understanding\n",
        "by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford et al 2018) explored a different approach, much more similar to what is typically done in computer vision.  In fine-tuning, we reuse the network architecture and simply replace the head.  We dont use any model specific architecture anymore, just a final layer.  There is an accompanying blog post [here](https://openai.com/blog/language-unsupervised/).  The image below is borrowed from that blog post\n",
        "\n",
        "![alt text](https://openai.com/content/images/2018/06/zero-shot-transfer@2x.png)\n",
        "\n",
        "As we can see from the images, these models can rapidly improve our downstream performance with very limited fine-tuning supervision.\n",
        "\n",
        "## The Transformer\n",
        "\n",
        "The original Transformer is an all-attention encoder-decoder model.  It is described at a high-level in [this Google AI post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).  This not really low-level enough to understand how it actually works.\n",
        "\n",
        "Here is an image of the model architecture for a Transformer:\n",
        "\n",
        "![Transformer Architecture](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)\n",
        "\n",
        "The reference implementation from Google is the [tensor2tensor repository](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor).  There is a lot going on in that codebase, which some people may find hard to follow.\n",
        "\n",
        "If you want to understand Transformers better, there is a terrific blog post called [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Rush 2018) where you can see how to code up a Transformer from scratch to do Neural Machine Translation (NMT) while following along with the paper.  \n",
        "\n",
        "In versions used in practice, there are slight differences from the actual image, most notably, that layer norm is performed first.  Also, in a causal LM pre-training setting, we have no need for the decoder, which simplifies our architecture substantially.  \n",
        "\n",
        "### Some implementation details\n",
        "\n",
        "#### Multi-headed attention and positional embeddings\n",
        "\n",
        "Multi-headed attention is one of the key innovations of the Transformer.  The idea was to allow each attention head to learn different relations between words as seen in this image from Google's blog:\n",
        "\n",
        "![MHA](https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png)\n",
        "\n",
        "For language modeling, as in the case of GPT, the decoder is entirely discarded, leaving only a masked self-attention in the encoder (this prevents us from seeing the future as we predict).\n",
        "\n",
        "![MHA Architecture](http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png)\n",
        "\n",
        "\n",
        "#### Positional embeddings\n",
        "\n",
        "To eliminate auto-regressive (RNN) models from the transformer, positional embeddings need to be created and added to the word embeddings.  Otherwise, during attention there would be no way to account for word position.  This is described in the blog post mentioned above.  There are 2 basic ways to support positional embeddings, sinusoidal and learned positional embeddings.\n",
        "\n",
        "\n",
        "#### A Transformer Encoder Layer\n",
        "\n",
        "Here is code adapted from [Baseline](https://github.com/dpressel/baseline) that implements a Transformer block used in a GPT-like architecture\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_heads, d_model, pdrop, scale=True, activation_type='relu', d_ff=None):\n",
        "        \"\"\"\n",
        "        :param num_heads (`int`): the number of heads for self-attention\n",
        "        :param d_model (`int`): The model dimension size\n",
        "        :param pdrop (`float`): The dropout probability\n",
        "        :param scale (`bool`): Whether we are doing scaled dot-product attention\n",
        "        :param activation_type: What activation type to use\n",
        "        :param d_ff: The feed forward layer size\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff if d_ff is not None else num_heads * d_model\n",
        "        self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale)\n",
        "        self.ffn = nn.Sequential(pytorch_linear(self.d_model, self.d_ff),\n",
        "                                 pytorch_activation(activation_type),\n",
        "                                 pytorch_linear(self.d_ff, self.d_model))\n",
        "        self.ln1 = nn.LayerNorm(self.d_model, eps=1e-12)\n",
        "        self.ln2 = nn.LayerNorm(self.d_model, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(pdrop)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        :param x: the inputs\n",
        "        :param mask: a mask for the inputs\n",
        "        :return: the encoder output\n",
        "        \"\"\"\n",
        "        # Builtin Attention mask\n",
        "        x = self.ln1(x)\n",
        "        h = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(h)\n",
        "\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.dropout(self.ffn(x))\n",
        "        return x\n",
        "\n",
        "```\n",
        "\n",
        "#### Multi-Headed Attention in PyTorch\n",
        "\n",
        "This operation is now built into PyTorch, with the limitation that only scaled-dot product attention is supported (Tensor2Tensor, Google's reference implementation supports both).  The code above does not use that module since it supports both scaled and unscaled attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "There is also an end-to-end example using the Baseline code above to train a GPT-like LM using the code above in PyTorch:\n",
        "\n",
        "https://github.com/dpressel/baseline/blob/master/api-examples/pretrain-transformer-lm.py\n",
        "\n",
        "\n",
        "\n",
        "## BERT\n",
        "\n",
        "For this section of the tutorial, we are going to fine-tune BERT [Devlin et al 2018](https://arxiv.org/abs/1810.04805), a transformer architecture that replaces the causal LM objective with 2 new objectives:\n",
        "\n",
        "1. Masking out words with some probability, predict the missing words\n",
        "\n",
        "![MLM](https://2.bp.blogspot.com/-pNxcHHXNZg0/W9iv3evVyOI/AAAAAAAADfA/KTSvKXNzzL0W8ry28PPl7nYI1CG_5WuvwCLcBGAs/s1600/f1.png)\n",
        "\n",
        "2. Given 2 adjacent sentences, predict if the second sentence follows the first\n",
        "\n",
        "![NSP](https://4.bp.blogspot.com/-K_7yu3kjF18/W9iv-R-MnyI/AAAAAAAADfE/xUwR_G1iTY0vq9X-Z3LnW5t4NLS9BQzdgCLcBGAs/s1600/f2.png)\n",
        "\n",
        "From an architecture diagram, [this blog post announcing BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) notes the differences:\n",
        "\n",
        "![BERT vs GPT and ELMo](https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s1600/image3.png)\n",
        "\n",
        "Our model will simply build on the existing model architecture with a single transformation layer to the output number of classes.  BERT is [open source](https://github.com/google-research/bert) but the code is in TensorFlow, and since this tutorial is written in PyTorch, we need a different solution.  We will use the [Hugging Face Transformer codebase](https://github.com/huggingface/pytorch-pretrained-BERT) as our API -- it can read in the original Google-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kx9qSNMgqVg",
        "colab_type": "code",
        "outputId": "27a424ed-ef27-4752-a40e-0c65653223b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Collecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.167)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.167)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1avGnWmjhbwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jwk_aHZfFa8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Tokenization in BERT\n",
        "\n",
        "In the last sequence, we talked about how ELMo biLMs can limit their parameters while accounting for unseen words using character-compositional word embeddings.  This technique is very powerful, but its also slow.  It is common in NMT to use some sort of sub-word encoding that limits the vocabulary size, but allows us to not have unattested words.  The `tensor2tensor` codebase, for example, creates an invertible encoding for words into sub-tokens with a limited voabulary.  The tokenizer is built from a corpus upfront and stored in a file, and then can be used to encode text.\n",
        "\n",
        "There are 4 phases in this algorithm described int the tensor2tensor codebase:\n",
        "\n",
        "\n",
        "    1. Tokenize into a list of tokens.  Each token is a unicode string of either\n",
        "      all alphanumeric characters or all non-alphanumeric characters.  We drop\n",
        "      tokens consisting of a single space that are between two alphanumeric\n",
        "      tokens.\n",
        "    2. Escape each token.  This escapes away special and out-of-vocabulary\n",
        "      characters, and makes sure that each token ends with an underscore, and\n",
        "      has no other underscores.\n",
        "    3. Represent each escaped token as a the concatenation of a list of subtokens\n",
        "      from the limited vocabulary.  Subtoken selection is done greedily from\n",
        "      beginning to end.  That is, we construct the list in order, always picking\n",
        "      the longest subtoken in our vocabulary that matches a prefix of the\n",
        "      remaining portion of the encoded token.\n",
        "    4. Concatenate these lists.  This concatenation is invertible due to the\n",
        "      fact that the trailing underscores indicate when one list is finished.\n",
        "\n",
        "\n",
        "On each iteration, all the words are segmented and counted, and the ones with high enough counts are stored in the new vocabulary.\n",
        "\n",
        "We can access Google's BERT Tokenizer via the Hugging Face API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR08ZqxRJpXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def whitespace_tokenizer(words):\n",
        "    return words.split() \n",
        "\n",
        "def sst2_tokenizer(words):\n",
        "    REPLACE = { \"'s\": \" 's \",\n",
        "                \"'ve\": \" 've \",\n",
        "                \"n't\": \" n't \",\n",
        "                \"'re\": \" 're \",\n",
        "                \"'d\": \" 'd \",\n",
        "                \"'ll\": \" 'll \",\n",
        "                \",\": \" , \",\n",
        "                \"!\": \" ! \",\n",
        "                }\n",
        "    words = words.lower()\n",
        "    words = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", words)\n",
        "    for k, v in REPLACE.items():\n",
        "            words = words.replace(k, v)\n",
        "    return [w.strip() for w in words.split()]\n",
        "\n",
        "BERT_TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "BERT_MODEL = BertModel.from_pretrained('bert-base-uncased')\n",
        "def bert_tokenizer(words, pretokenizer=whitespace_tokenizer):\n",
        "    subwords = ['[CLS]']\n",
        "    for word in pretokenizer(words):\n",
        "        if word == '<unk>':\n",
        "            subword = '[UNK]'\n",
        "        else:\n",
        "            subword = BERT_TOKENIZER.tokenize(word)\n",
        "        subwords += subword\n",
        "    return subwords + ['[SEP]']\n",
        "\n",
        "def bert_vectorizer(sentence):\n",
        "    return BERT_TOKENIZER.convert_tokens_to_ids(sentence)\n",
        "    #return [BERT_TOKENIZER.vocab.get(subword, BERT_TOKENIZER.vocab['[PAD]']) for subword in sentence]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8w_daEBhnXP",
        "colab_type": "text"
      },
      "source": [
        "Our model this time around is very simple.  It has an output linear layer that comes from pooled output from BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7nF-N93hmFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class FineTuneClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, num_classes, embed_dim, hidden_units=[]):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        input_units = embed_dim\n",
        "        output_units = embed_dim\n",
        "        sequence = []\n",
        "        for h in hidden_units:\n",
        "            sequence.append(nn.Linear(input_units, h))\n",
        "            input_units = h\n",
        "            output_units = h\n",
        "            \n",
        "        sequence.append(nn.Linear(output_units, num_classes))\n",
        "        self.outputs = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x, lengths = inputs\n",
        "        \n",
        "        input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1)\n",
        "        input_type_ids = torch.zeros(x.shape, device=x.device, dtype=torch.long)\n",
        "        _, pooled = self.base_model(x, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
        "        \n",
        "        stacked = self.outputs(pooled)\n",
        "        return F.log_softmax(stacked, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04KqbE6RiIvI",
        "colab_type": "text"
      },
      "source": [
        "All the rest of our code comes from the previous sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSltrQlhiNBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import codecs\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, num_classes, embed_dims, rnn_units, rnn_layers=1, dropout=0.5, hidden_units=[]):\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = torch.nn.LSTM(embed_dims,\n",
        "                                 rnn_units,\n",
        "                                 rnn_layers,\n",
        "                                 dropout=dropout,\n",
        "                                 bidirectional=False,\n",
        "                                 batch_first=False)\n",
        "        nn.init.orthogonal_(self.rnn.weight_hh_l0)\n",
        "        nn.init.orthogonal_(self.rnn.weight_ih_l0)\n",
        "        sequence = []\n",
        "        input_units = rnn_units\n",
        "        output_units = rnn_units\n",
        "        for h in hidden_units:\n",
        "            sequence.append(nn.Linear(input_units, h))\n",
        "            input_units = h\n",
        "            output_units = h\n",
        "            \n",
        "        sequence.append(nn.Linear(output_units, num_classes))\n",
        "        self.outputs = nn.Sequential(*sequence)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        one_hots, lengths = inputs\n",
        "        embed = self.dropout(self.embeddings(one_hots))\n",
        "        embed = embed.transpose(0, 1)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.tolist())\n",
        "        _, hidden = self.rnn(packed)\n",
        "        hidden = hidden[0].view(hidden[0].shape[1:])\n",
        "        linear = self.outputs(hidden)\n",
        "        return F.log_softmax(linear, dim=-1)\n",
        "\n",
        "class ConfusionMatrix:\n",
        "    \"\"\"Confusion matrix with metrics\n",
        "\n",
        "    This class accumulates classification output, and tracks it in a confusion matrix.\n",
        "    Metrics are available that use the confusion matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, labels):\n",
        "        \"\"\"Constructor with input labels\n",
        "\n",
        "        :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n",
        "        \"\"\"\n",
        "        if type(labels) is dict:\n",
        "            self.labels = []\n",
        "            for i in range(len(labels)):\n",
        "                self.labels.append(labels[i])\n",
        "        else:\n",
        "            self.labels = labels\n",
        "        nc = len(self.labels)\n",
        "        self._cm = np.zeros((nc, nc), dtype=np.int)\n",
        "\n",
        "    def add(self, truth, guess):\n",
        "        \"\"\"Add a single value to the confusion matrix based off `truth` and `guess`\n",
        "\n",
        "        :param truth: The real `y` value (or ground truth label)\n",
        "        :param guess: The guess for `y` value (or assertion)\n",
        "        \"\"\"\n",
        "\n",
        "        self._cm[truth, guess] += 1\n",
        "\n",
        "    def __str__(self):\n",
        "        values = []\n",
        "        width = max(8, max(len(x) for x in self.labels) + 1)\n",
        "        for i, label in enumerate([''] + self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "        values += ['\\n']\n",
        "        for i, label in enumerate(self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "            for j in range(len(self.labels)):\n",
        "                values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\n",
        "            values += ['\\n']\n",
        "        values += ['\\n']\n",
        "        return ''.join(values)\n",
        "\n",
        "    def save(self, outfile):\n",
        "        ordered_fieldnames = OrderedDict([(\"labels\", None)] + [(l, None) for l in self.labels])\n",
        "        with open(outfile, 'w') as f:\n",
        "            dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)\n",
        "            dw.writeheader()\n",
        "            for index, row in enumerate(self._cm):\n",
        "                row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n",
        "                row_dict.update({\"labels\": self.labels[index]})\n",
        "                dw.writerow(row_dict)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the matrix\n",
        "        \"\"\"\n",
        "        self._cm *= 0\n",
        "\n",
        "    def get_correct(self):\n",
        "        \"\"\"Get the diagonals of the confusion matrix\n",
        "\n",
        "        :return: (``int``) Number of correct classifications\n",
        "        \"\"\"\n",
        "        return self._cm.diagonal().sum()\n",
        "\n",
        "    def get_total(self):\n",
        "        \"\"\"Get total classifications\n",
        "\n",
        "        :return: (``int``) total classifications\n",
        "        \"\"\"\n",
        "        return self._cm.sum()\n",
        "\n",
        "    def get_acc(self):\n",
        "        \"\"\"Get the accuracy\n",
        "\n",
        "        :return: (``float``) accuracy\n",
        "        \"\"\"\n",
        "        return float(self.get_correct())/self.get_total()\n",
        "\n",
        "    def get_recall(self):\n",
        "        \"\"\"Get the recall\n",
        "\n",
        "        :return: (``float``) recall\n",
        "        \"\"\"\n",
        "        total = np.sum(self._cm, axis=1)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_support(self):\n",
        "        return np.sum(self._cm, axis=1)\n",
        "\n",
        "    def get_precision(self):\n",
        "        \"\"\"Get the precision\n",
        "        :return: (``float``) precision\n",
        "        \"\"\"\n",
        "\n",
        "        total = np.sum(self._cm, axis=0)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_mean_precision(self):\n",
        "        \"\"\"Get the mean precision across labels\n",
        "\n",
        "        :return: (``float``) mean precision\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_precision())\n",
        "\n",
        "    def get_weighted_precision(self):\n",
        "        return np.sum(self.get_precision() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_mean_recall(self):\n",
        "        \"\"\"Get the mean recall across labels\n",
        "\n",
        "        :return: (``float``) mean recall\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_recall())\n",
        "\n",
        "    def get_weighted_recall(self):\n",
        "        return np.sum(self.get_recall() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_weighted_f(self, beta=1):\n",
        "        return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_macro_f(self, beta=1):\n",
        "        \"\"\"Get the macro F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) macro F_b\n",
        "        \"\"\"\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        return np.mean(self.get_class_f(beta))\n",
        "\n",
        "    def get_class_f(self, beta=1):\n",
        "        p = self.get_precision()\n",
        "        r = self.get_recall()\n",
        "\n",
        "        b = beta*beta\n",
        "        d = (b * p + r)\n",
        "        d = (d == 0) + d\n",
        "\n",
        "        return (b + 1) * p * r / d\n",
        "\n",
        "    def get_f(self, beta=1):\n",
        "        \"\"\"Get 2 class F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) 2-class F_b\n",
        "        \"\"\"\n",
        "        p = self.get_precision()[1]\n",
        "        r = self.get_recall()[1]\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        d = (beta*beta * p + r)\n",
        "        if d == 0:\n",
        "            return 0\n",
        "        return (beta*beta + 1) * p * r / d\n",
        "\n",
        "    def get_all_metrics(self):\n",
        "        \"\"\"Make a map of metrics suitable for reporting, keyed by metric name\n",
        "\n",
        "        :return: (``dict``) Map of metrics keyed by metric names\n",
        "        \"\"\"\n",
        "        metrics = {'acc': self.get_acc()}\n",
        "        # If 2 class, assume second class is positive AKA 1\n",
        "        if len(self.labels) == 2:\n",
        "            metrics['precision'] = self.get_precision()[1]\n",
        "            metrics['recall'] = self.get_recall()[1]\n",
        "            metrics['f1'] = self.get_f(1)\n",
        "        else:\n",
        "            metrics['mean_precision'] = self.get_mean_precision()\n",
        "            metrics['mean_recall'] = self.get_mean_recall()\n",
        "            metrics['macro_f1'] = self.get_macro_f(1)\n",
        "            metrics['weighted_precision'] = self.get_weighted_precision()\n",
        "            metrics['weighted_recall'] = self.get_weighted_recall()\n",
        "            metrics['weighted_f1'] = self.get_weighted_f(1)\n",
        "        return metrics\n",
        "\n",
        "    def add_batch(self, truth, guess):\n",
        "        \"\"\"Add a batch of data to the confusion matrix\n",
        "\n",
        "        :param truth: The truth tensor\n",
        "        :param guess: The guess tensor\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for truth_i, guess_i in zip(truth, guess):\n",
        "            self.add(truth_i, guess_i)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, optimizer: torch.optim.Optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def run(self, model, labels, train, loss, batch_size): \n",
        "        model.train()       \n",
        "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        cm = ConfusionMatrix(labels)\n",
        "\n",
        "        for batch in train_loader:\n",
        "            loss_value, y_pred, y_actual = self.update(model, loss, batch)\n",
        "            _, best = y_pred.max(1)\n",
        "            yt = y_actual.cpu().int().numpy()\n",
        "            yp = best.cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "\n",
        "        print(cm.get_all_metrics())\n",
        "        return cm\n",
        "    \n",
        "    def update(self, model, loss, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "        x, lengths, y = batch\n",
        "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[perm_idx]\n",
        "        y_sorted = y[perm_idx]\n",
        "        y_sorted = y_sorted.to('cuda:0')\n",
        "        inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "        y_pred = model(inputs)\n",
        "        loss_value = loss(y_pred, y_sorted)\n",
        "        loss_value.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss_value.item(), y_pred, y_sorted\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def run(self, model, labels, dataset, batch_size=1):\n",
        "        model.eval()\n",
        "        valid_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "        cm = ConfusionMatrix(labels)\n",
        "        for batch in valid_loader:\n",
        "            y_pred, y_actual = self.inference(model, batch)\n",
        "            _, best = y_pred.max(1)\n",
        "            yt = y_actual.cpu().int().numpy()\n",
        "            yp = best.cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "        return cm\n",
        "\n",
        "    def inference(self, model, batch):\n",
        "        with torch.no_grad():\n",
        "            x, lengths, y = batch\n",
        "            lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "            x_sorted = x[perm_idx]\n",
        "            y_sorted = y[perm_idx]\n",
        "            y_sorted = y_sorted.to('cuda:0')\n",
        "            inputs = (x_sorted.to('cuda:0'), lengths)\n",
        "            y_pred = model(inputs)\n",
        "            return y_pred, y_sorted\n",
        "\n",
        "def fit(model, labels, optimizer, loss, epochs, batch_size, train, valid, test):\n",
        "\n",
        "    trainer = Trainer(optimizer)\n",
        "    evaluator = Evaluator()\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print('EPOCH {}'.format(epoch + 1))\n",
        "        print('=================================')\n",
        "        print('Training Results')\n",
        "        cm = trainer.run(model, labels, train, loss, batch_size)\n",
        "        print('Validation Results')\n",
        "        cm = evaluator.run(model, labels, valid)\n",
        "        print(cm.get_all_metrics())\n",
        "        if cm.get_acc() > best_acc:\n",
        "            print('New best model {:.2f}'.format(cm.get_acc()))\n",
        "            best_acc = cm.get_acc()\n",
        "            torch.save(model.state_dict(), './checkpoint.pth')\n",
        "    if test:\n",
        "        model.load_state_dict(torch.load('./checkpoint.pth'))\n",
        "        cm = evaluator.run(model, labels, test)\n",
        "        print('Final result')\n",
        "        print(cm.get_all_metrics())\n",
        "    return cm.get_acc()\n",
        "\n",
        "def whitespace_tokenizer(words: str) -> List[str]:\n",
        "    return words.split() \n",
        "\n",
        "def sst2_tokenizer(words: str) -> List[str]:\n",
        "    REPLACE = { \"'s\": \" 's \",\n",
        "                \"'ve\": \" 've \",\n",
        "                \"n't\": \" n't \",\n",
        "                \"'re\": \" 're \",\n",
        "                \"'d\": \" 'd \",\n",
        "                \"'ll\": \" 'll \",\n",
        "                \",\": \" , \",\n",
        "                \"!\": \" ! \",\n",
        "                }\n",
        "    words = words.lower()\n",
        "    words = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", words)\n",
        "    for k, v in REPLACE.items():\n",
        "            words = words.replace(k, v)\n",
        "    return [w.strip() for w in words.split()]\n",
        "\n",
        "\n",
        "class Reader:\n",
        "\n",
        "    def __init__(self, files, lowercase=True, min_freq=0,\n",
        "                 tokenizer=sst2_tokenizer, vectorizer=None):\n",
        "        self.lowercase = lowercase\n",
        "        self.tokenizer = tokenizer\n",
        "        build_vocab = vectorizer is None\n",
        "        self.vectorizer = vectorizer if vectorizer else self._vectorizer\n",
        "        x = Counter()\n",
        "        y = Counter()\n",
        "        for file_name in files:\n",
        "            if file_name is None:\n",
        "                continue\n",
        "            with codecs.open(file_name, encoding='utf-8', mode='r') as f:\n",
        "                for line in f:\n",
        "                    words = line.split()\n",
        "                    y.update(words[0])\n",
        "\n",
        "                    if build_vocab:\n",
        "                        words = self.tokenizer(' '.join(words[1:]))\n",
        "                        words = words if not self.lowercase else [w.lower() for w in words]\n",
        "                        x.update(words)\n",
        "        self.labels = list(y.keys())\n",
        "\n",
        "        if build_vocab:\n",
        "            x = dict(filter(lambda cnt: cnt[1] >= min_freq, x.items()))\n",
        "            alpha = list(x.keys())\n",
        "            alpha.sort()\n",
        "            self.vocab = {w: i+1 for i, w in enumerate(alpha)}\n",
        "            self.vocab['[PAD]'] = 0\n",
        "\n",
        "        self.labels.sort()\n",
        "\n",
        "    def _vectorizer(self, words: List[str]) -> List[int]:\n",
        "        return [self.vocab.get(w, 0) for w in words]\n",
        "\n",
        "    def load(self, filename: str) -> TensorDataset:\n",
        "        label2index = {l: i for i, l in enumerate(self.labels)}\n",
        "        xs = []\n",
        "        lengths = []\n",
        "        ys = []\n",
        "        with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
        "            for line in f:\n",
        "                words = line.split()\n",
        "                ys.append(label2index[words[0]])\n",
        "                words = self.tokenizer(' '.join(words[1:]))\n",
        "                words = words if not self.lowercase else [w.lower() for w in words]\n",
        "                vec = self.vectorizer(words)\n",
        "                lengths.append(len(vec))\n",
        "                xs.append(torch.tensor(vec, dtype=torch.long))\n",
        "        x_tensor = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
        "        lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "        y_tensor = torch.tensor(ys, dtype=torch.long)\n",
        "        return TensorDataset(x_tensor, lengths_tensor, y_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfdjPng9jiyX",
        "colab_type": "text"
      },
      "source": [
        "Lets use the trec dataset again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viWkpJPrjlT5",
        "colab_type": "code",
        "outputId": "e1bd35c8-aa35-4ef7-c34f-35d676e94803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/08km2ean8bkt7p3/trec.tar.gz?dl=1\n",
        "!tar -xzf 'trec.tar.gz?dl=1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-26 16:11:50--  https://www.dropbox.com/s/08km2ean8bkt7p3/trec.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/08km2ean8bkt7p3/trec.tar.gz [following]\n",
            "--2019-06-26 16:11:50--  https://www.dropbox.com/s/dl/08km2ean8bkt7p3/trec.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com/cd/0/get/AjmRZa8cUvcuVud0HfCm25tSr9rrBzwSvSRsT0YCnBmLX4ugEjzxDPrCXQdEZy6yHGcaBEzjLoQWBNMq9O8mFrLwnYbovya9ZcerCXxewB_lYg/file?dl=1# [following]\n",
            "--2019-06-26 16:11:50--  https://uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com/cd/0/get/AjmRZa8cUvcuVud0HfCm25tSr9rrBzwSvSRsT0YCnBmLX4ugEjzxDPrCXQdEZy6yHGcaBEzjLoQWBNMq9O8mFrLwnYbovya9ZcerCXxewB_lYg/file?dl=1\n",
            "Resolving uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com (uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com (uc07ceac62805b51d07f217610fe.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117253 (115K) [application/binary]\n",
            "Saving to: ‘trec.tar.gz?dl=1’\n",
            "\n",
            "trec.tar.gz?dl=1    100%[===================>] 114.50K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-06-26 16:11:51 (1.26 MB/s) - ‘trec.tar.gz?dl=1’ saved [117253/117253]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnkvVi5Wjxek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE = 'trec'\n",
        "TRAIN = os.path.join(BASE, 'trec.nodev.utf8')\n",
        "VALID = os.path.join(BASE, 'trec.dev.utf8')\n",
        "TEST = os.path.join(BASE, 'trec.test.utf8')\n",
        "\n",
        "\n",
        "\n",
        "r = Reader((TRAIN, VALID, TEST,), lowercase=False, vectorizer=bert_vectorizer, tokenizer=bert_tokenizer)\n",
        "train = r.load(TRAIN)\n",
        "valid = r.load(VALID)\n",
        "test = r.load(TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHqndauRjmKC",
        "colab_type": "code",
        "outputId": "60fb57ee-36a6-4f2e-d301-aee00c717c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "bert_small_dims = 768\n",
        "batch_size = 50\n",
        "epochs = 12\n",
        "model = FineTuneClassifier(BERT_MODEL, len(r.labels), bert_small_dims)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model has {num_params} parameters\") \n",
        "\n",
        "\n",
        "model.to('cuda:0')\n",
        "loss = torch.nn.NLLLoss()\n",
        "loss = loss.to('cuda:0')\n",
        "\n",
        "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(learnable_params, lr=1.0e-4)\n",
        "\n",
        "fit(model, r.labels, optimizer, loss, epochs, batch_size, train, valid, test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has 109486854 parameters\n",
            "EPOCH 1\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.8492, 'mean_precision': 0.8590416631560661, 'mean_recall': 0.784499104767959, 'macro_f1': 0.8093044230616595, 'weighted_precision': 0.8533096832063289, 'weighted_recall': 0.8492, 'weighted_f1': 0.848636551495934}\n",
            "Validation Results\n",
            "{'acc': 0.9292035398230089, 'mean_precision': 0.9095790095790096, 'mean_recall': 0.8812630270963604, 'macro_f1': 0.8914806712112505, 'weighted_precision': 0.9282319649576285, 'weighted_recall': 0.9292035398230089, 'weighted_f1': 0.9275440576086902}\n",
            "New best model 0.93\n",
            "EPOCH 2\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9628, 'mean_precision': 0.9524731788585094, 'mean_recall': 0.9448956967584476, 'macro_f1': 0.9484990826490849, 'weighted_precision': 0.9626990548505957, 'weighted_recall': 0.9628, 'weighted_f1': 0.9627032294356912}\n",
            "Validation Results\n",
            "{'acc': 0.9292035398230089, 'mean_precision': 0.8638110106824105, 'mean_recall': 0.8815288595309299, 'macro_f1': 0.871157144636212, 'weighted_precision': 0.9305201649233531, 'weighted_recall': 0.9292035398230089, 'weighted_f1': 0.9284064322652922}\n",
            "EPOCH 3\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9808, 'mean_precision': 0.9719354668161183, 'mean_recall': 0.9671642710385303, 'macro_f1': 0.9694792291565891, 'weighted_precision': 0.9808098720024074, 'weighted_recall': 0.9808, 'weighted_f1': 0.9807847593714502}\n",
            "Validation Results\n",
            "{'acc': 0.9336283185840708, 'mean_precision': 0.8613646571368058, 'mean_recall': 0.8812111801242235, 'macro_f1': 0.8694841800034229, 'weighted_precision': 0.9355869215265541, 'weighted_recall': 0.9336283185840708, 'weighted_f1': 0.9338809099577255}\n",
            "New best model 0.93\n",
            "EPOCH 4\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9848, 'mean_precision': 0.9776029488297443, 'mean_recall': 0.9779103190581798, 'macro_f1': 0.977749185848333, 'weighted_precision': 0.9848002110592166, 'weighted_recall': 0.9848, 'weighted_f1': 0.9847911908528922}\n",
            "Validation Results\n",
            "{'acc': 0.9292035398230089, 'mean_precision': 0.8913693076567979, 'mean_recall': 0.8783466489559001, 'macro_f1': 0.884055962958923, 'weighted_precision': 0.9289369849696025, 'weighted_recall': 0.9292035398230089, 'weighted_f1': 0.9287924452101446}\n",
            "EPOCH 5\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9848, 'mean_precision': 0.9854122143464519, 'mean_recall': 0.9838835226480714, 'macro_f1': 0.9846379431601209, 'weighted_precision': 0.9847945539830835, 'weighted_recall': 0.9848, 'weighted_f1': 0.9847931410582056}\n",
            "Validation Results\n",
            "{'acc': 0.9424778761061947, 'mean_precision': 0.88095885250736, 'mean_recall': 0.9103293235022014, 'macro_f1': 0.8921774029882935, 'weighted_precision': 0.9456838207581173, 'weighted_recall': 0.9424778761061947, 'weighted_f1': 0.943389530970516}\n",
            "New best model 0.94\n",
            "EPOCH 6\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9914, 'mean_precision': 0.991118960568874, 'mean_recall': 0.991319848415677, 'macro_f1': 0.9912189003908601, 'weighted_precision': 0.9913974585501834, 'weighted_recall': 0.9914, 'weighted_f1': 0.9913981912040757}\n",
            "Validation Results\n",
            "{'acc': 0.9358407079646017, 'mean_precision': 0.865366231447053, 'mean_recall': 0.8828034983573286, 'macro_f1': 0.8719543320948618, 'weighted_precision': 0.938526465559395, 'weighted_recall': 0.9358407079646017, 'weighted_f1': 0.9359954593101597}\n",
            "EPOCH 7\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.99, 'mean_precision': 0.9922550327263108, 'mean_recall': 0.9922480579393076, 'macro_f1': 0.9922497570471602, 'weighted_precision': 0.9900052504656903, 'weighted_recall': 0.99, 'weighted_f1': 0.9900003235077307}\n",
            "Validation Results\n",
            "{'acc': 0.9446902654867256, 'mean_precision': 0.8895784877999916, 'mean_recall': 0.9120300037743102, 'macro_f1': 0.8988087850900407, 'weighted_precision': 0.9466095102184958, 'weighted_recall': 0.9446902654867256, 'weighted_f1': 0.9450749223132502}\n",
            "New best model 0.94\n",
            "EPOCH 8\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9948, 'mean_precision': 0.992081693594972, 'mean_recall': 0.9918503627928592, 'macro_f1': 0.9919649278309528, 'weighted_precision': 0.9948054321154616, 'weighted_recall': 0.9948, 'weighted_f1': 0.9948014769958603}\n",
            "Validation Results\n",
            "{'acc': 0.9269911504424779, 'mean_precision': 0.8457595583573675, 'mean_recall': 0.8756029295972739, 'macro_f1': 0.8564792695313505, 'weighted_precision': 0.9320081653020288, 'weighted_recall': 0.9269911504424779, 'weighted_f1': 0.9287012057045636}\n",
            "EPOCH 9\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9928, 'mean_precision': 0.9882900644171505, 'mean_recall': 0.9902469247169735, 'macro_f1': 0.9892619119579414, 'weighted_precision': 0.992805816521421, 'weighted_recall': 0.9928, 'weighted_f1': 0.9928020645313087}\n",
            "Validation Results\n",
            "{'acc': 0.9424778761061947, 'mean_precision': 0.8879027928725677, 'mean_recall': 0.8873502211064701, 'macro_f1': 0.88742745925785, 'weighted_precision': 0.9427350675934334, 'weighted_recall': 0.9424778761061947, 'weighted_f1': 0.9423744559242012}\n",
            "EPOCH 10\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.997, 'mean_precision': 0.9955228606452602, 'mean_recall': 0.9916502652921401, 'macro_f1': 0.9935598026431807, 'weighted_precision': 0.9969958572199921, 'weighted_recall': 0.997, 'weighted_f1': 0.9969953102410011}\n",
            "Validation Results\n",
            "{'acc': 0.9314159292035398, 'mean_precision': 0.8598396399049993, 'mean_recall': 0.9006049337659446, 'macro_f1': 0.8734440270707258, 'weighted_precision': 0.9371022713509264, 'weighted_recall': 0.9314159292035398, 'weighted_f1': 0.933331001008043}\n",
            "EPOCH 11\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9966, 'mean_precision': 0.9954256743587128, 'mean_recall': 0.9954362773046639, 'macro_f1': 0.9954309073939234, 'weighted_precision': 0.9966000625429468, 'weighted_recall': 0.9966, 'weighted_f1': 0.996599940911514}\n",
            "Validation Results\n",
            "{'acc': 0.9469026548672567, 'mean_precision': 0.8719894668438752, 'mean_recall': 0.912716274422834, 'macro_f1': 0.8857442104529586, 'weighted_precision': 0.9529740729221685, 'weighted_recall': 0.9469026548672567, 'weighted_f1': 0.9492268965209602}\n",
            "New best model 0.95\n",
            "EPOCH 12\n",
            "=================================\n",
            "Training Results\n",
            "{'acc': 0.9924, 'mean_precision': 0.9827262161825051, 'mean_recall': 0.9862213472022652, 'macro_f1': 0.9844450146177012, 'weighted_precision': 0.9924476903230014, 'weighted_recall': 0.9924, 'weighted_f1': 0.9924156596167579}\n",
            "Validation Results\n",
            "{'acc': 0.9402654867256637, 'mean_precision': 0.8839045260037711, 'mean_recall': 0.9086306334324311, 'macro_f1': 0.8943936076549379, 'weighted_precision': 0.9418194401421545, 'weighted_recall': 0.9402654867256637, 'weighted_f1': 0.9406143269844134}\n",
            "Final result\n",
            "{'acc': 0.966, 'mean_precision': 0.9465838972284876, 'mean_recall': 0.9696421762849385, 'macro_f1': 0.9562200813212646, 'weighted_precision': 0.9668990380196144, 'weighted_recall': 0.966, 'weighted_f1': 0.9658308408340981}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.966"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1XaO_0fm5M0",
        "colab_type": "text"
      },
      "source": [
        "We can see that this is a *massive* gain over our CNN baseline and also improves over our ELMo contextual embeddings for this dataset.  BERT has been shown high-performance results across many datasets, and integrating it into unstructured prediction problems is quite simple, as we saw in this section.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this section we investigated the Transformer model architecture, particularly in the context of pretraining LMs.  We discussed some of the model details and we looked at how BERT extends the GPT approach from OpenAI.  We then built our own fine-tuned classifier using the Hugging Face PyTorch library to create and re-load the BERT model and add our own layers on top.\n",
        "\n"
      ]
    }
  ]
}